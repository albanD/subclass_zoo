{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e5c6028",
   "metadata": {},
   "source": [
    "what is pytorch?\n",
    "  - library for machine learning\n",
    "  - kernels + automatic differentiation\n",
    "  - optimization\n",
    "      resnet(image) -> classification\n",
    "      resnet(Tensor) -> Tensor\n",
    "\n",
    "      actual_result = resnet(image)\n",
    "      loss = (actual_result - expected_result).sum()\n",
    "\n",
    "      f(image)(p0, p1, ... pn) = loss\n",
    "      dp0/loss ... dpn/loss  <~~ HOW???\n",
    "  - how is AD implemented in PyTorch?\n",
    "      reverse mode automatic differentiation\n",
    "      \n",
    "      sin, cos, tanh, matmul, linear, ...\n",
    "      \n",
    "      x = f(p)\n",
    "      y = g(x)\n",
    "      z = h(y)\n",
    "      \n",
    "      grad_y = h_backward(grad_z)\n",
    "      grad_x = g_backward(grad_y)\n",
    "      grad_p = f_backward(grad_x)\n",
    "\n",
    "\n",
    "what is functorch?\n",
    "\n",
    "\n",
    "what is it do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ede1e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bf9d79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0462f9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1cb2ffc9",
   "metadata": {},
   "source": [
    "This notebook walks through a self-contained implementation of\n",
    "functorch, including support for both vjp and vmap combinators (using\n",
    "PyTorch only to implement primitive tensor operations).  It follows\n",
    "the tradition of\n",
    "[Autodidax](https://jax.readthedocs.io/en/latest/autodidax.html) (a\n",
    "pedagogical reimplementation of JAX, the library functorch is inspired\n",
    "by) and [Simple\n",
    "Autograd](https://colab.research.google.com/drive/1VpeE6UvEPRz9HmsHh1KS0XxXjYu533EC?usp=sharing)\n",
    "(Zachary Devito's pedagogical reimplementation of autograd, which the\n",
    "autograd system in this notebook is based off of.) You can [open this\n",
    "file in\n",
    "Colab](https://colab.research.google.com/github/albanD/subclass_zoo/blob/main/simple_functorch.ipynb)\n",
    "and play around with the examples.\n",
    "\n",
    "As a simplified implementation of functorch, this notebook also makes\n",
    "it easier to investigate some more subtle aspects of how PyTorch's\n",
    "native autograd system interacts with composable transforms.  In\n",
    "particular, we will see that PyTorch's native implementation of double\n",
    "backwards (which shares the same tape through multiple levels of\n",
    "differentiation) differs from functorch's nested grad implementation\n",
    "(which maintains a separate tape per level)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed150b0",
   "metadata": {},
   "source": [
    "To get started, we replicate some of the data structures and helper functions\n",
    "from Simple Autograd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba464220",
   "metadata": {
    "lines_to_end_of_cell_marker": 2
   },
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import functools\n",
    "from dataclasses import dataclass\n",
    "from typing import Callable, Dict, List, NamedTuple, Optional\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "class TapeEntry(NamedTuple):\n",
    "    # names of the inputs to the original computation\n",
    "    inputs: List[str]\n",
    "    # names of the outputs of the original computation\n",
    "    outputs: List[str]\n",
    "    # apply chain rule\n",
    "    propagate: Callable[[List[Tensor]], List[Tensor]]\n",
    "\n",
    "\n",
    "_name = 0\n",
    "\n",
    "\n",
    "def fresh_name() -> str:\n",
    "    \"\"\"create a new unique name for a variable: v0, v1, v2\"\"\"\n",
    "    global _name\n",
    "    r = f\"v{_name}\"\n",
    "    _name += 1\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af3ca0b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "This is a little helper function for converting the dim argument in\n",
    "sum into an explicit list of dimensions that will be reduced over.\n",
    "It takes the dim of the tensor we are summing over and the dim\n",
    "argument itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "836a1635",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def sum_dims(*, input_dim, dim):\n",
    "    if dim is None:\n",
    "        return tuple(range(0, input_dim))\n",
    "    elif isinstance(dim, int):\n",
    "        return (dim,)\n",
    "    else:\n",
    "        return tuple(sorted(dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f74fb48",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In Simple Autograd, we provided a Variable wrapper class which\n",
    "provided a traditional Tensor style interface for our objects; in\n",
    "functorch proper, objects are repeatedly wrapped in this way to\n",
    "implement multipler layers of transformations.\n",
    "\n",
    "In my opinion, this sort of wrapper makes it more difficult to\n",
    "understand the flow of logic.  So in Simple Functorch, we take a\n",
    "different approach: we won't make use of a wrapper class at all,\n",
    "instead showing how to add it in the end as syntax sugar on top of our\n",
    "system.\n",
    "\n",
    "For debuggability purposes, however, it is nice to have a way to\n",
    "identify variables by a human readable name.  We'll do this by setting\n",
    "a t_name attribute on PyTorch tensors whenever we allocate a new\n",
    "tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9d17aef",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def label(t: Tensor, name: str = None):\n",
    "    if not hasattr(t, \"t_name\"):\n",
    "        t.t_name = name or fresh_name()\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84d79ce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.add(torch.tensor([0.0, 0.1]), torch.tensor([0.3, 0.5])).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd046576",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "So if we aren't going to have a wrapper around each tensor, how will\n",
    "we actually implement our logic?  We will organize our various layers\n",
    "of transformations as separate Dispatcher objects, which define\n",
    "methods for performing operations on tensors, but are not Tensors\n",
    "themselves.  For example, instead of defining Tensor.add(Tensor), we\n",
    "will define Dispatcher.add(Tensor, Tensor).  If you are familiar with\n",
    "historical ATen, in the original implementation of ATen, these\n",
    "correspond to the CPUType/CUDAType/VariableType objects from that\n",
    "implementation (this was replaced with the modern dispatcher as the\n",
    "original vtable-based implementation did not support adding custom\n",
    "operators.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59a69d9d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class Dispatcher:\n",
    "    inner = None\n",
    "\n",
    "    def mul(self, lhs, rhs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def add(self, lhs, rhs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # Sum has been generalized to take an optional dim argument, which we\n",
    "    # will need for Batched tensors\n",
    "    def sum(self, input, dim=None, name=None):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def expand(self, input, sizes):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # For closure under Batched tensors, we need these operations...\n",
    "    def unsqueeze(self, input, dim):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def squeeze(self, input, dim):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # ...and we also need to overload the meaning of size/ones to\n",
    "    # hide/reinsert batch dimensions.  We also introduce a concept\n",
    "    # of \"lifting\" a tensor to be batched by broadcasting it on\n",
    "    # a dimension\n",
    "    def size(self, input):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def ones(self, size):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def lift(self, input, d):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # For convenience, we provide dim, which just returns the length of\n",
    "    # the sizes\n",
    "    def dim(self, input):\n",
    "        return len(self.size(input))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e21984",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "To start with, we can implement a backend dispatcher layer Torch,\n",
    "which just forwards the operator calls to our underlying library PyTorch\n",
    "(and ensures that all the allocated tensors are labeled with variable).  You could\n",
    "also imagine replacing this with a Numpy backend or even a pure Python\n",
    "variant (although this file is not currently setup to do so.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31f94dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27101073",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class Torch(Dispatcher):\n",
    "    def mul(self, lhs, rhs):\n",
    "        return label(torch.mul(lhs, rhs))\n",
    "\n",
    "    def add(self, lhs, rhs):\n",
    "        return label(torch.add(lhs, rhs))\n",
    "\n",
    "    def sum(self, input, dim=None, name=None):\n",
    "        if dim is None:\n",
    "            return label(torch.sum(input), name)\n",
    "        else:\n",
    "            return label(torch.sum(input, dim), name)\n",
    "\n",
    "    def expand(self, input, sizes):\n",
    "        return label(input.expand(sizes))\n",
    "\n",
    "    def unsqueeze(self, input, dim):\n",
    "        return label(torch.unsqueeze(input, dim))\n",
    "\n",
    "    def squeeze(self, input, dim):\n",
    "        return label(torch.squeeze(input, dim))\n",
    "\n",
    "    def size(self, input):\n",
    "        # Return size a tuple for marginally more compact printing\n",
    "        assert isinstance(input, torch.Tensor)\n",
    "        return tuple(input.size())\n",
    "\n",
    "    def ones(self, size):\n",
    "        return label(torch.ones(size))\n",
    "\n",
    "    def custom_vjp(self, fwd_fn, bwd_fn, *args):\n",
    "        # The backend layer for custom_vjp just calls fwd_fn.\n",
    "        # Why doesn't it create an autograd.Function? We're assuming the backend\n",
    "        # layer doesn't need to handle Autograd.\n",
    "        a, b = fwd_fn(self, *args)\n",
    "        result = label(a), label(b)\n",
    "        return result\n",
    "\n",
    "    def lift(self, input, d):\n",
    "        assert d is self\n",
    "        return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51b9d2f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Torch()\n",
    "x.add(torch.tensor(0.3), torch.tensor(0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e428f835",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Dispatcher layers are composable via object composition: we can\n",
    "imagine a stack of dispatchers, each one calling into the next.\n",
    "For example, the Logger dispatcher simply prints out what operation\n",
    "was called on it, and then forwards on the operation to the inner\n",
    "dispatcher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1500269c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_vjp_str(r, fwd_fn, bwd_fn, args):\n",
    "    arg_names = \", \".join([a.t_name for a in args])\n",
    "    r_is_tensor = isinstance(r, torch.Tensor)\n",
    "    if r_is_tensor:\n",
    "        result_names = r.t_name\n",
    "    else:\n",
    "        result_names = [r.t_name for r in r]\n",
    "        if len(result_names) == 1:\n",
    "            result_names = f\"{result_names[0]},\"\n",
    "        else:\n",
    "            result_names = \", \".join(result_names)\n",
    "\n",
    "    print(\n",
    "        f\"{result_names} = custom_vjp({fwd_fn.__name__}, {bwd_fn.__name__}, {arg_names})\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd6436a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger(Dispatcher):\n",
    "    def __init__(self, inner, *, name):\n",
    "        self.inner = inner\n",
    "        self.name = f\"  {name}\"\n",
    "\n",
    "    def size(self, input):\n",
    "        # don't log size calls\n",
    "        return self.inner.size(input)\n",
    "\n",
    "    def ones(self, size):\n",
    "        r = self.inner.ones(size)\n",
    "        print(f\"{self.name} {r.t_name}: {self.size(r)} = ones({size})\")\n",
    "        return r\n",
    "\n",
    "    def mul(self, lhs, rhs):\n",
    "        r = self.inner.mul(lhs, rhs)\n",
    "        if isinstance(rhs, float):\n",
    "            print(f\"{self.name} {r.t_name}: {self.size(r)} = {lhs.t_name} * {rhs}\")\n",
    "        else:\n",
    "            print(\n",
    "                f\"{self.name} {r.t_name}: {self.size(r)} = {lhs.t_name} * {rhs.t_name}\"\n",
    "            )\n",
    "        return r\n",
    "\n",
    "    def add(self, lhs, rhs):\n",
    "        r = self.inner.add(lhs, rhs)\n",
    "        print(f\"{self.name} {r.t_name}: {self.size(r)} = {lhs.t_name} + {rhs.t_name}\")\n",
    "        return r\n",
    "\n",
    "    def sum(self, input, dim=None, name=None):\n",
    "        r = self.inner.sum(input, dim=dim, name=name)\n",
    "        print(f\"{self.name} {r.t_name}: {self.size(r)} = {input.t_name}.sum(dim={dim})\")\n",
    "        return r\n",
    "\n",
    "    def unsqueeze(self, input, dim):\n",
    "        r = self.inner.unsqueeze(input, dim)\n",
    "        print(\n",
    "            f\"{self.name} {r.t_name}: {self.size(r)} = {input.t_name}.unsqueeze({dim})\"\n",
    "        )\n",
    "        return r\n",
    "\n",
    "    def squeeze(self, input, dim):\n",
    "        r = self.inner.squeeze(input, dim)\n",
    "        print(f\"{self.name} {r.t_name}: {self.size(r)} = {input.t_name}.squeeze({dim})\")\n",
    "        return r\n",
    "\n",
    "    def expand(self, input, sizes):\n",
    "        r = self.inner.expand(input, sizes)\n",
    "        print(\n",
    "            f\"{self.name} {r.t_name}: {self.size(r)} = {input.t_name}.expand({sizes})\"\n",
    "        )\n",
    "        return r\n",
    "\n",
    "    def custom_vjp(self, fwd_fn, bwd_fn, *args):\n",
    "        r = self.inner.custom_vjp(fwd_fn, bwd_fn, *args)\n",
    "        print(custom_vjp_str(r, fwd_fn, bwd_fn, args))\n",
    "        return r\n",
    "\n",
    "    def lift(self, input, d):\n",
    "        if d is self:\n",
    "            return input\n",
    "        return self.inner.lift(input, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e323b552",
   "metadata": {},
   "source": [
    "Here is a simple example of using Logger and Torch together.  Whenever\n",
    "we make calls to operations, we must do so via the Dispatcher object.\n",
    "We will explicitly write out all of these calls before we add wrapper\n",
    "class sugaring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26b4c5e5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Torch v4: (2,) = ones(2)\n",
      "  Torch2 v4: (2,) = ones(2)\n",
      "  Torch v5: (2,) = ones(2)\n",
      "  Torch2 v5: (2,) = ones(2)\n",
      "  Torch v6: (2,) = v4 + v5\n",
      "  Torch2 v6: (2,) = v4 + v5\n",
      "tensor([2., 2.])\n"
     ]
    }
   ],
   "source": [
    "d = Logger(Logger(Torch(), name=\"Torch\"), name=\"Torch2\")\n",
    "print(d.add(d.ones(2), d.ones(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcd422b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "With the Dispatcher structure in hand, we are now in a good place to\n",
    "port the autograd implementation from Simple Autograd into our new\n",
    "framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "54293b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autograd(Dispatcher):\n",
    "    # create_graph here corresponds to the create_graph kwarg in traditional\n",
    "    # PyTorch, which controls whether or not the graph of the derivative\n",
    "    # will be constructed, allowing computing higher order derivatives.\n",
    "    # We will see that although create_graph=True allows Autograd to directly\n",
    "    # support higher order derivatives, layering an Autograd to another\n",
    "    # Autograd will also allow higher order derivatives.\n",
    "    def __init__(self, inner, *, name=\"Autograd\", create_graph: bool = False):\n",
    "        self.inner = inner\n",
    "        self.gradient_tape = []\n",
    "        self.name = name\n",
    "        self.create_graph = create_graph\n",
    "\n",
    "    # create_graph controls where add/mul/etc calls from the backwards\n",
    "    # propagators go: if you create_graph, they recursively call back to\n",
    "    # the current Autograd dispatcher; otherwise they move on to the inner\n",
    "    # layer.\n",
    "    def backward_inner(self):\n",
    "        if self.create_graph:\n",
    "            return self\n",
    "        else:\n",
    "            return self.inner\n",
    "\n",
    "    def mul(self, lhs, rhs):\n",
    "        if isinstance(rhs, float) and rhs == 1.0:\n",
    "            # peephole optimization\n",
    "            return lhs\n",
    "\n",
    "        # define forward\n",
    "        # first, run the operation in the inner layer to get the initial\n",
    "        # result\n",
    "        r = self.inner.mul(lhs, rhs)\n",
    "        # We directly implement printing here as it indicates whether or not\n",
    "        # this operation was saved to the tape or not\n",
    "        print(f\"{self.name} {r.t_name}: {self.size(r)} = {lhs.t_name} * {rhs.t_name}\")\n",
    "\n",
    "        # record what the inputs and outputs of the op were\n",
    "        inputs = [lhs.t_name, rhs.t_name]\n",
    "        outputs = [r.t_name]\n",
    "\n",
    "        # define backprop\n",
    "        \n",
    "        # \n",
    "        # lhs = f(p)\n",
    "        # rhs = g(p)\n",
    "        # outputs = lhs * rhs\n",
    "        # L = rest_of_network(outputs)\n",
    "        #\n",
    "        # dL/dp\n",
    "        # dL/dlhs, dL/drhs\n",
    "        \n",
    "        # r = lhs * rhs\n",
    "        # dr/dlhs = rhs\n",
    "        # dr/drhs = lhs\n",
    "        #\n",
    "        # outputs = lhs * rhs\n",
    "        # L = rest_of_network(outputs)\n",
    "        #\n",
    "        # dL/doutputs = dL/dr\n",
    "        # dL/dinputs = dL/dlhs, dL/drhs\n",
    "        #\n",
    "        # dL/dlhs = dL/dr * dr/dlhs\n",
    "        def propagate(dL_doutputs: List[Tensor]):\n",
    "            (dL_dr,) = dL_doutputs\n",
    "\n",
    "            dr_dlhs = rhs  # partial derivative of r = lhs*rhs\n",
    "            dr_drhs = lhs  # partial derivative of r = lhs*rhs\n",
    "\n",
    "            # chain rule propagation from outputs to inputs of multiply.\n",
    "            # Notice that the propagation rule may itself call\n",
    "            # other operations; depending on create_graph, they may\n",
    "            # either go to self or self.inner; self.backward_inner()\n",
    "            # controls which one we go to.\n",
    "            dL_dlhs = self.backward_inner().mul(dL_dr, dr_dlhs)\n",
    "            dL_drhs = self.backward_inner().mul(dL_dr, dr_drhs)\n",
    "            dL_dinputs = [dL_dlhs, dL_drhs]\n",
    "            return dL_dinputs\n",
    "\n",
    "        # finally, we record the compute we did on the tape\n",
    "        self.gradient_tape.append(\n",
    "            TapeEntry(inputs=inputs, outputs=outputs, propagate=propagate)\n",
    "        )\n",
    "        return r\n",
    "\n",
    "    # The rest of the implementations follow in the same way and can\n",
    "    # be skipped\n",
    "\n",
    "    def add(self, lhs, rhs):\n",
    "        # Add follows a similar pattern to Mul, but it doesn't end up\n",
    "        # capturing any variables.\n",
    "        r = self.inner.add(lhs, rhs)\n",
    "        print(f\"{self.name} {r.t_name}: {self.size(r)} = {lhs.t_name} + {rhs.t_name}\")\n",
    "\n",
    "        def propagate(dL_doutputs: List[Tensor]):\n",
    "            (dL_dr,) = dL_doutputs\n",
    "            dr_dlhs = 1.0\n",
    "            dr_drhs = 1.0\n",
    "            dL_dlhs = self.backward_inner().mul(dL_dr, dr_dlhs)\n",
    "            dL_drhs = self.backward_inner().mul(dL_dr, dr_drhs)\n",
    "            return [dL_dlhs, dL_drhs]\n",
    "\n",
    "        self.gradient_tape.append(\n",
    "            TapeEntry(\n",
    "                inputs=[lhs.t_name, rhs.t_name], outputs=[r.t_name], propagate=propagate\n",
    "            )\n",
    "        )\n",
    "        return r\n",
    "\n",
    "    # Extended to handle dim argument for Batched (later)\n",
    "    def sum(self, input: Tensor, dim=None, name: Optional[str] = None):\n",
    "        r = self.inner.sum(input, dim=dim, name=name)\n",
    "        print(f\"{self.name} {r.t_name}: {self.size(r)} = {input.t_name}.sum(dim={dim})\")\n",
    "\n",
    "        def propagate(dL_doutputs: List[Tensor]):\n",
    "            (dL_dr,) = dL_doutputs\n",
    "            size = self.inner.size(input)\n",
    "            res = dL_dr\n",
    "            # Broadcast over all dimensions that were reduced over\n",
    "            for i in sum_dims(input_dim=self.inner.dim(input), dim=dim):\n",
    "                res = self.backward_inner().unsqueeze(res, i)\n",
    "            return [self.backward_inner().expand(res, size)]\n",
    "\n",
    "        self.gradient_tape.append(\n",
    "            TapeEntry(inputs=[input.t_name], outputs=[r.t_name], propagate=propagate)\n",
    "        )\n",
    "        return r\n",
    "\n",
    "    # Unlike Simple Autograd, this expand requires the input to have\n",
    "    # been unsqueezed before hand.  This lets us avoid having to do\n",
    "    # at::sum_to for the nontrivial case (which is more complicated)\n",
    "    def expand(self, input: Tensor, sizes: List[int]):\n",
    "        assert self.inner.dim(input) == len(sizes)  # only works if dims match\n",
    "        r = self.inner.expand(input, sizes)\n",
    "        print(\n",
    "            f\"{self.name} {r.t_name}: {self.size(r)} = {input.t_name}.expand({sizes})\"\n",
    "        )\n",
    "\n",
    "        def propagate(dL_doutputs: List[Tensor]):\n",
    "            (dL_dr,) = dL_doutputs\n",
    "            input_size = self.inner.size(input)\n",
    "            dims = tuple(\n",
    "                i for i in range(self.inner.dim(input)) if input_size[i] != sizes[i]\n",
    "            )\n",
    "            # We wanted a sum keepdim=True, but I didn't want to force\n",
    "            # everyone to support it so manually unsqueeze\n",
    "            res = self.backward_inner().sum(dL_dr, dims)\n",
    "            for d in dims:\n",
    "                res = self.backward_inner().unsqueeze(res, d)\n",
    "            return [res]\n",
    "\n",
    "        self.gradient_tape.append(\n",
    "            TapeEntry(inputs=[input.t_name], outputs=[r.t_name], propagate=propagate)\n",
    "        )\n",
    "        return r\n",
    "\n",
    "    # Unsqueeze are required for sum backwards, and then squeeze is required\n",
    "    # for closure.  Size needed for batched tensor to modify size.\n",
    "\n",
    "    def size(self, input: Tensor):\n",
    "        return self.inner.size(input)\n",
    "\n",
    "    def squeeze(self, input: Tensor, dim):\n",
    "        r = self.inner.squeeze(input, dim)\n",
    "        print(\n",
    "            f\"{self.name} {r.t_name}: {self.size(r)} = {input.t_name}.squeeze(dim={dim})\"\n",
    "        )\n",
    "\n",
    "        def propagate(dL_doutputs: List[Tensor]):\n",
    "            (dL_dr,) = dL_outputs\n",
    "            return [self.backward_inner().unsqueeze(dL_dr, dim)]\n",
    "\n",
    "        self.gradient_tape.append(\n",
    "            TapeEntry(inputs=[input.t_name], outputs=[r.t_name], propagate=propagate)\n",
    "        )\n",
    "        return r\n",
    "\n",
    "    def unsqueeze(self, input: Tensor, dim):\n",
    "        r = self.inner.unsqueeze(input, dim)\n",
    "        print(\n",
    "            f\"{self.name} {r.t_name}: {self.size(r)} = {input.t_name}.unsqueeze(dim={dim})\"\n",
    "        )\n",
    "\n",
    "        def propagate(dL_doutputs: List[Tensor]):\n",
    "            (dL_dr,) = dL_doutputs\n",
    "            return [self.backward_inner().squeeze(dL_dr, dim)]\n",
    "\n",
    "        self.gradient_tape.append(\n",
    "            TapeEntry(inputs=[input.t_name], outputs=[r.t_name], propagate=propagate)\n",
    "        )\n",
    "        return r\n",
    "\n",
    "    def ones(self, size):\n",
    "        return self.inner.ones(size)\n",
    "\n",
    "    def custom_vjp(self, fwd_fn, bwd_fn, *args):\n",
    "        # To support Autograd(Autograd(Torch()), custom_vjp MUST call custom_vjp\n",
    "        # on the inner dispatcher. If it instead called fwd_fn(*args), then\n",
    "        # the inner Autograd dispatcher would not use bwd_fn in its backward pass.\n",
    "        r, saved = self.inner.custom_vjp(fwd_fn, bwd_fn, *args)\n",
    "        print(custom_vjp_str(r, fwd_fn, bwd_fn, args))\n",
    "\n",
    "        # To preserve custom backward semantics, we create a lambda that calls\n",
    "        # bwd_fn. This lambda is then saved on the gradient tape.\n",
    "        def propagate(dL_doutputs: List[Tensor]):\n",
    "            return bwd_fn(self, dL_doutputs, saved)\n",
    "\n",
    "        self.gradient_tape.append(\n",
    "            TapeEntry(\n",
    "                inputs=[arg.t_name for arg in args],\n",
    "                outputs=[r.t_name],\n",
    "                propagate=propagate,\n",
    "            )\n",
    "        )\n",
    "        return r, saved\n",
    "\n",
    "    def lift(self, input, d):\n",
    "        if d is self:\n",
    "            return input\n",
    "        return self.inner.lift(input, d)\n",
    "\n",
    "    def grad(self, L, desired_results: List[Tensor]) -> List[Tensor]:\n",
    "        # this map holds dL/dX for all values X\n",
    "        dL_d: Dict[str, Tensor] = {}\n",
    "        # It starts by initializing the 'seed' dL/dL, which is 1\n",
    "        # TODO: indirect this via the backend\n",
    "        dL_d[L.t_name] = self.inner.ones(())\n",
    "        print(f\"-- {self.name} d{L.t_name} -------\")\n",
    "\n",
    "        # look up dL_dentries. If a variable is never used to compute the loss,\n",
    "        # we consider its gradient None, see the note below about zeros for more information.\n",
    "        def gather_grad(entries: List[str]):\n",
    "            return [dL_d[entry] if entry in dL_d else None for entry in entries]\n",
    "\n",
    "        \"\"\"\n",
    "      x = f(p)\n",
    "      y = g(x)\n",
    "      z = h(y)\n",
    "      \n",
    "      gradient_tape = [\n",
    "          TapeEntry(\n",
    "              inputs=[\"p\"],\n",
    "              outputs=[\"x\"],\n",
    "              propagate=f_backward\n",
    "          ),\n",
    "          g_backward,\n",
    "          h_backward\n",
    "      ]\n",
    "      \n",
    "      grad_y = h_backward(grad_z)\n",
    "      grad_x = g_backward(grad_y)\n",
    "      grad_p = f_backward(grad_x)\n",
    "        \"\"\"\n",
    "        \n",
    "        # propagate the gradient information backward\n",
    "        for entry in reversed(self.gradient_tape):\n",
    "            dL_doutputs = gather_grad(entry.outputs)\n",
    "            if all(dL_doutput is None for dL_doutput in dL_doutputs):\n",
    "                # optimize for the case where some gradient pathways are zero. See\n",
    "                # The note below for more details.\n",
    "                continue\n",
    "            # perform chain rule propagation specific to each compute\n",
    "            dL_dinputs = entry.propagate(dL_doutputs)\n",
    "\n",
    "            # Accululate the gradient produced for each input.\n",
    "            # Each use of a variable produces some gradient dL_dinput for that\n",
    "            # use. The multivariate chain rule tells us it is safe to sum\n",
    "            # all the contributions together.\n",
    "            for input, dL_dinput in zip(entry.inputs, dL_dinputs):\n",
    "                if input not in dL_d:\n",
    "                    dL_d[input] = dL_dinput\n",
    "                else:\n",
    "                    dL_d[input] = self.backward_inner().add(dL_d[input], dL_dinput)\n",
    "\n",
    "        # print some information to understand the values of each intermediate\n",
    "        # for name, value in dL_d.items():\n",
    "        #     print(f'{self.name} d{L.t_name}_d{name} = {value.t_name}')\n",
    "        print(f\"------------------------\")\n",
    "\n",
    "        return gather_grad(desired.t_name for desired in desired_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901c6b4a",
   "metadata": {},
   "source": [
    "To calculate some simple gradients, we can compose Autograd with\n",
    "Torch and get the result we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b725c318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a tensor([0.4963])\n",
      "b tensor([0.7682])\n",
      "Autograd v83: (1,) = v81 + v82\n",
      "Autograd v84: (1,) = v83 * v82\n",
      "-- Autograd dv84 -------\n",
      "TapeEntry(inputs=['v83', 'v82'], outputs=['v84'], propagate=<function Autograd.mul.<locals>.propagate at 0x116501700>)\n",
      "TapeEntry(inputs=['v81', 'v82'], outputs=['v83'], propagate=<function Autograd.add.<locals>.propagate at 0x1145d78b0>)\n",
      "------------------------\n",
      "da tensor([0.7682])\n",
      "db tensor([2.0327])\n",
      "b+a+b tensor([2.0327])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "a, b = label(torch.rand(1)), label(torch.rand(1))\n",
    "\n",
    "print(\"a\", a)\n",
    "print(\"b\", b)\n",
    "\n",
    "# f(a, b) = (a + b) * b = L\n",
    "# dL/da = ((a+b)*b)/da = d(a+b)/da * b + (a+b)*db/da = b\n",
    "# dL/db = d(a+b)/db * b + (a+b)*db/db = b + a + b\n",
    "#\n",
    "# dL/dL = 1.0\n",
    "#\n",
    "# L = t * b\n",
    "# dL/dt += b\n",
    "# dL/db += (t aka a + b)\n",
    "#\n",
    "# t = a + b\n",
    "# dt/da = 1\n",
    "# dt/db = 1\n",
    "#\n",
    "# dL/da += dL/dt * dt/da = b\n",
    "# dL/db += dL/dt * dt/db\n",
    "#\n",
    "# final values:\n",
    "# dL/da = b\n",
    "# dL/db = a + b + a\n",
    "\n",
    "def simple(d, a, b):\n",
    "    t = d.add(a, b)\n",
    "    return d.mul(t, b)\n",
    "\n",
    "\n",
    "d = Autograd(Torch())\n",
    "\n",
    "loss = simple(d, a, b)\n",
    "da, db = d.grad(loss, [a, b])\n",
    "print(\"da\", da)\n",
    "print(\"db\", db)\n",
    "print(\"b+a+b\", b+a+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd64990",
   "metadata": {},
   "source": [
    "To compute higher order gradients, we have two options.  First,\n",
    "we can do traditional PyTorch style higher order differentiation\n",
    "with `create_graph=True`, writing the backpropagation computations directly\n",
    "into the tape so they can be further differentiated over.  This is also\n",
    "what the original Simple Autograd implementation does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e1a5342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autograd v120: (1,) = v81 + v82\n",
      "Autograd v121: (1,) = v120 * v82\n",
      "Autograd L0: () = v121.sum(dim=None)\n",
      "-- Autograd dL0 -------\n",
      "Autograd v123: (1,) = v122.unsqueeze(dim=0)\n",
      "Autograd v124: (1,) = v123.expand((1,))\n",
      "Autograd v125: (1,) = v124 * v82\n",
      "Autograd v126: (1,) = v124 * v120\n",
      "Autograd v127: (1,) = v126 + v125\n",
      "------------------------\n",
      "Autograd v128: (1,) = v125 * v125\n",
      "Autograd v129: (1,) = v127 * v127\n",
      "Autograd v130: (1,) = v128 + v129\n",
      "Autograd L1: () = v130.sum(dim=None)\n",
      "-- Autograd dL1 -------\n",
      "Autograd v132: (1,) = v131.unsqueeze(dim=0)\n",
      "Autograd v133: (1,) = v132.expand((1,))\n",
      "Autograd v134: (1,) = v133 * v127\n",
      "Autograd v135: (1,) = v133 * v127\n",
      "Autograd v136: (1,) = v134 + v135\n",
      "Autograd v137: (1,) = v133 * v125\n",
      "Autograd v138: (1,) = v133 * v125\n",
      "Autograd v139: (1,) = v137 + v138\n",
      "Autograd v140: (1,) = v139 + v136\n",
      "Autograd v141: (1,) = v136 * v120\n",
      "Autograd v142: (1,) = v136 * v124\n",
      "Autograd v143: (1,) = v140 * v82\n",
      "Autograd v144: (1,) = v140 * v124\n",
      "Autograd v145: (1,) = v141 + v143\n",
      "Autograd v146: () = v145.sum(dim=())\n",
      "Autograd v147: () = v146.squeeze(dim=0)\n",
      "Autograd v148: (1,) = v144 + v142\n",
      "------------------------\n",
      "da tensor([4.0654])\n",
      "db tensor([9.6672])\n"
     ]
    }
   ],
   "source": [
    "d = Autograd(Torch(), create_graph=True)\n",
    "\n",
    "# I slightly generalized this function so that it works for the next\n",
    "# example; d2 is the dispatcher run on the first grad call, and d1 is\n",
    "# for the second (we'll see why the numbers are inverted shortly).\n",
    "def run_gradients(d2, d1):\n",
    "    # our first loss\n",
    "    L0 = d2.sum(simple(d2, a, b), name=\"L0\")\n",
    "\n",
    "    # compute derivatives of our inputs\n",
    "    dL0_da, dL0_db = d2.grad(L0, [a, b])\n",
    "\n",
    "    # In real code, how would we switch from executing from d2 to d1?\n",
    "    # In functorch, the d2 dispatch calls would happen in the inside of\n",
    "    # a higher-order grad() call; when we exit from this call, all\n",
    "    # of the involved tensors are unwrapped.\n",
    "\n",
    "    # now lets compute the L2 norm of our derivatives\n",
    "    L1 = d1.sum(d1.add(d1.mul(dL0_da, dL0_da), d1.mul(dL0_db, dL0_db)), name=\"L1\")\n",
    "\n",
    "    # and take the gradient of that.\n",
    "    # notice there are two losses involved1.\n",
    "    dL1_da, dL1_db = d1.grad(L1, [a, b])\n",
    "    return dL1_da, dL1_db\n",
    "\n",
    "\n",
    "da, db = run_gradients(d, d)\n",
    "print(\"da\", da)\n",
    "print(\"db\", db)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b17d02c",
   "metadata": {},
   "source": [
    "Our second option is to follow functorch's implementation strategy, which\n",
    "is to stack two Autograd dispatchers on top of each other.  Here, it is\n",
    "not necessary to `create_graph=True`, because when the backpropagator forwards\n",
    "to the inner dispatcher, it will record those operations on the tape too.\n",
    "But if you look at the output, you will notice something very interesting:\n",
    "the first portion of the tape is exactly replicated between Autograd1 and\n",
    "Autograd2: we're duplicating the tape in this case!  So PyTorch's default\n",
    "implementation of backwards is more efficient, because it avoids having to\n",
    "record the tape twice (although this doesn't matter too much, because the\n",
    "saved tensors themselves can be shared between the two tapes, so it is just\n",
    "the operator graph that is duplicated.\n",
    "\n",
    "This is our first example of using two dispatchers.  While we are\n",
    "performing the inner grad, we perform our operations on the outer\n",
    "dispatcher `d2`; after we are done with the inner grad we switch to\n",
    "`d1`.  Intuitively, this corresponds from passing out of the inner\n",
    "`grad` call to the outer `grad` call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a7c47f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Torch v149: (1,) = v81 + v82\n",
      "Autograd1 v149: (1,) = v81 + v82\n",
      "Autograd2 v149: (1,) = v81 + v82\n",
      "  Torch v150: (1,) = v149 * v82\n",
      "Autograd1 v150: (1,) = v149 * v82\n",
      "Autograd2 v150: (1,) = v149 * v82\n",
      "  Torch L0: () = v150.sum(dim=None)\n",
      "Autograd1 L0: () = v150.sum(dim=None)\n",
      "Autograd2 L0: () = v150.sum(dim=None)\n",
      "  Torch v151: () = ones(())\n",
      "-- Autograd2 dL0 -------\n",
      "  Torch v152: (1,) = v151.unsqueeze(0)\n",
      "Autograd1 v152: (1,) = v151.unsqueeze(dim=0)\n",
      "  Torch v153: (1,) = v152.expand((1,))\n",
      "Autograd1 v153: (1,) = v152.expand((1,))\n",
      "  Torch v154: (1,) = v153 * v82\n",
      "Autograd1 v154: (1,) = v153 * v82\n",
      "  Torch v155: (1,) = v153 * v149\n",
      "Autograd1 v155: (1,) = v153 * v149\n",
      "  Torch v156: (1,) = v155 + v154\n",
      "Autograd1 v156: (1,) = v155 + v154\n",
      "------------------------\n",
      "  Torch v157: (1,) = v154 * v154\n",
      "Autograd1 v157: (1,) = v154 * v154\n",
      "  Torch v158: (1,) = v156 * v156\n",
      "Autograd1 v158: (1,) = v156 * v156\n",
      "  Torch v159: (1,) = v157 + v158\n",
      "Autograd1 v159: (1,) = v157 + v158\n",
      "  Torch L1: () = v159.sum(dim=None)\n",
      "Autograd1 L1: () = v159.sum(dim=None)\n",
      "  Torch v160: () = ones(())\n",
      "-- Autograd1 dL1 -------\n",
      "  Torch v161: (1,) = v160.unsqueeze(0)\n",
      "  Torch v162: (1,) = v161.expand((1,))\n",
      "  Torch v163: (1,) = v162 * 1.0\n",
      "  Torch v164: (1,) = v162 * 1.0\n",
      "  Torch v165: (1,) = v164 * v156\n",
      "  Torch v166: (1,) = v164 * v156\n",
      "  Torch v167: (1,) = v165 + v166\n",
      "  Torch v168: (1,) = v163 * v154\n",
      "  Torch v169: (1,) = v163 * v154\n",
      "  Torch v170: (1,) = v168 + v169\n",
      "  Torch v171: (1,) = v167 * 1.0\n",
      "  Torch v172: (1,) = v167 * 1.0\n",
      "  Torch v173: (1,) = v170 + v172\n",
      "  Torch v174: (1,) = v171 * v149\n",
      "  Torch v175: (1,) = v171 * v153\n",
      "  Torch v176: (1,) = v173 * v82\n",
      "  Torch v177: (1,) = v173 * v153\n",
      "  Torch v178: (1,) = v174 + v176\n",
      "  Torch v179: () = v178.sum(dim=())\n",
      "  Torch v180: () = v179.squeeze(0)\n",
      "  Torch v181: (1,) = v175 * 1.0\n",
      "  Torch v182: (1,) = v175 * 1.0\n",
      "  Torch v183: (1,) = v177 + v182\n",
      "------------------------\n",
      "da tensor([4.0654])\n",
      "db tensor([9.6672])\n"
     ]
    }
   ],
   "source": [
    "# turning off create_graph will impede us from seeing the logging lines for\n",
    "# the second backwards, so we turn on logging for Torch to see them\n",
    "d1 = Autograd(Logger(Torch(), name=\"Torch\"), name=\"Autograd1\", create_graph=False)\n",
    "d2 = Autograd(d1, name=\"Autograd2\", create_graph=False)\n",
    "\n",
    "da, db = run_gradients(d2, d1)\n",
    "print(\"da\", da)\n",
    "print(\"db\", db)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f343af81",
   "metadata": {},
   "source": [
    "Under what situations might it be profitable to keep the two tapes separate?\n",
    "One guess we might have is if there is another functional transformation\n",
    "wedged between the two autograd transformations.  We would then expect the\n",
    "backwards formula we save to be different between the two tapes.  To do this, I\n",
    "first need to implement batched tensors.\n",
    "\n",
    "One unusual thing about this implementation is that we do not need to wrap\n",
    "tensors to change their sizes; instead, we just override the meaning of\n",
    "size() on the dispatcher to hide batch dimensions.  One case we do not\n",
    "exercise in this example is implicit broadcasting when you combine a tensor\n",
    "that is not batched with a tensor that is batched: without wrappers, a user\n",
    "must explicitly lift (e.g., unsqueeze and expand) tensors they wish to\n",
    "replicate across the batch dimension.  The code below will blindly attempt to\n",
    "reinterpret a tensor as a batched tensor, even when it may not make sense (if\n",
    "there is a size mismatch, however, you will get an assert failure).  Similarly,\n",
    "once you exit a vmap region, all previously vmap'ed tensors \"magically\" become\n",
    "unbatched.  functorch did not pursue this implementation because at the time\n",
    "Tensor.size() was not virtual and thus it was not possible to override (this\n",
    "will be changing soon)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "dd1d0688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This implementation of Batched only supports inserting a dimension\n",
    "# at the very front\n",
    "class Batched(Dispatcher):\n",
    "    def __init__(self, inner, *, length, name=\"Batched\"):\n",
    "        self.inner = inner\n",
    "        self.name = name\n",
    "        self.length = length\n",
    "\n",
    "    def size(self, input):\n",
    "        sizes = self.inner.size(input)\n",
    "        assert sizes[0] == self.length\n",
    "        return sizes[1:]\n",
    "\n",
    "    def ones(self, size):\n",
    "        return self.inner.ones((self.length,) + size)\n",
    "\n",
    "    def mul(self, lhs, rhs):\n",
    "        assert self.inner.size(lhs)[0] == self.length\n",
    "        if not isinstance(rhs, float):\n",
    "            assert self.inner.size(rhs)[0] == self.length\n",
    "        return self.inner.mul(lhs, rhs)\n",
    "\n",
    "    def add(self, lhs, rhs):\n",
    "        assert self.inner.size(lhs)[0] == self.length\n",
    "        assert self.inner.size(rhs)[0] == self.length\n",
    "        return self.inner.add(lhs, rhs)\n",
    "\n",
    "    def sum(self, input, dim=None, name=None):\n",
    "        # offset all the summed over dimensions by one\n",
    "        assert self.inner.size(input)[0] == self.length\n",
    "        dim = tuple(\n",
    "            i + 1 for i in sum_dims(input_dim=self.inner.dim(input) - 1, dim=dim)\n",
    "        )\n",
    "        return self.inner.sum(input, dim, name=name)\n",
    "\n",
    "    def expand(self, input, sizes):\n",
    "        # offset sizes by one\n",
    "        assert self.inner.size(input)[0] == self.length\n",
    "        return self.inner.expand(input, (self.inner.size(input)[0],) + sizes)\n",
    "\n",
    "    def squeeze(self, input, dim):\n",
    "        # offset dim by one\n",
    "        assert self.inner.size(input)[0] == self.length\n",
    "        return self.inner.squeeze(input, dim + 1)\n",
    "\n",
    "    def unsqueeze(self, input, dim):\n",
    "        # offset dim by one\n",
    "        assert self.inner.size(input)[0] == self.length\n",
    "        return self.inner.unsqueeze(input, dim + 1)\n",
    "\n",
    "    def custom_vjp(self, fwd_fn, bwd_fn, *args):\n",
    "        def batchify(fn):\n",
    "            def new_fn(d, *args):\n",
    "                new_d = Batched(d, length=self.length, name=\"GeneratedBatched\")\n",
    "                result = fn(new_d, *args)\n",
    "                return result\n",
    "\n",
    "            return new_fn\n",
    "\n",
    "        # If we have Batched(Autograd(Torch()), then we would like the inner\n",
    "        # dispatcher to receive a call to custom_vjp so that it preserves the\n",
    "        # backward semantics. However, since this is the Batched dispatcher,\n",
    "        # we want the innermost Torch dispatcher to run a batched version of fwd_fn\n",
    "        # function! The way we get this to work is to create a new fwd_fn, that,\n",
    "        # when executed, executes a batched version of fwd_fn.\n",
    "        #\n",
    "        # Same thing for the bwd_fn.\n",
    "        # NB: currently simple_functorch assumes that all Tensors are batched at\n",
    "        # dimension 0. I'm not sure how this logic would look like without\n",
    "        # this assumption (in functorch tensors may not be batched).\n",
    "        r, saved = self.inner.custom_vjp(batchify(fwd_fn), batchify(bwd_fn), *args)\n",
    "        return r, saved\n",
    "\n",
    "    # The lift operation takes a tensor associated with some inner\n",
    "    # dispatcher, and \"lifts\" it so that it is interpreted neutrally\n",
    "    # for the outer dispatcher.  For most dispatchers this is trivial,\n",
    "    # but for batched tensor it is not: given a tensor x, to interpret\n",
    "    # it as x under the Batching dispatcher, we have to expand it so\n",
    "    # that it is broadcasted along its first dimension.\n",
    "    def lift(self, input, d):\n",
    "        if d is self:\n",
    "            return input\n",
    "        b_input = self.inner.unsqueeze(input, 0)\n",
    "        b_input = self.inner.expand(b_input, (self.length,) + self.inner.size(input))\n",
    "        return self.inner.lift(b_input, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "dffb9a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autograd1 v81: (2, 4) = v79 + v80\n",
      "Autograd3 v81: (4,) = v79 + v80\n",
      "Autograd1 v82: (2, 4) = v81 * v80\n",
      "Autograd3 v82: (4,) = v81 * v80\n",
      "Autograd1 L0: (2,) = v82.sum(dim=(1,))\n",
      "Autograd3 L0: () = v82.sum(dim=0)\n",
      "-- Autograd3 dL0 -------\n",
      "Autograd1 v84: (2, 1) = v83.unsqueeze(dim=1)\n",
      "Autograd1 v85: (2, 4) = v84.expand((2, 4))\n",
      "Autograd1 v86: (2, 4) = v85 * v80\n",
      "Autograd1 v87: (2, 4) = v85 * v81\n",
      "Autograd1 v88: (2, 4) = v87 + v86\n",
      "------------------------\n",
      "Autograd1 v89: (2, 4) = v86 * v86\n",
      "Autograd1 v90: (2, 4) = v88 * v88\n",
      "Autograd1 v91: (2, 4) = v89 + v90\n",
      "Autograd1 L1: () = v91.sum(dim=None)\n",
      "-- Autograd1 dL1 -------\n",
      "------------------------\n",
      "va tensor([[0.4556, 0.6323, 0.3489, 0.4017],\n",
      "        [0.0223, 0.1689, 0.2939, 0.5185]])\n",
      "vb tensor([[0.6977, 0.8000, 0.1610, 0.2823],\n",
      "        [0.6816, 0.9152, 0.3971, 0.8742]])\n",
      "dva tensor([[3.7019, 4.4647, 1.3419, 1.9325],\n",
      "        [2.7711, 3.9985, 2.1762, 4.5337]])\n",
      "dvb tensor([[ 8.7992, 10.5293,  3.0059,  4.4296],\n",
      "        [ 6.9054,  9.8274,  5.1466, 10.8156]])\n"
     ]
    }
   ],
   "source": [
    "# Our inputs are batched this time!\n",
    "va, vb = label(torch.rand(2, 4)), label(torch.rand(2, 4))\n",
    "\n",
    "d1 = Autograd(Torch(), name=\"Autograd1\", create_graph=False)\n",
    "d2 = Batched(d1, length=2, name=\"Batched2\")\n",
    "d3 = Autograd(d2, name=\"Autograd3\", create_graph=False)\n",
    "\n",
    "\n",
    "def run_batched_gradients(d3, d2, d1):\n",
    "    # our first loss\n",
    "    # we write the dimension we reduce on explicitly for clarity\n",
    "    L0 = d3.sum(simple(d3, va, vb), dim=0, name=\"L0\")\n",
    "\n",
    "    # compute derivatives of our inputs\n",
    "    dL0_da, dL0_db = d3.grad(L0, [va, vb])\n",
    "\n",
    "    # now lets compute the L2 norm of our derivatives\n",
    "    L1 = d1.sum(d1.add(d1.mul(dL0_da, dL0_da), d1.mul(dL0_db, dL0_db)), name=\"L1\")\n",
    "\n",
    "    # and take the gradient of that.\n",
    "    # notice there are two losses involved1.\n",
    "    dL1_da, dL1_db = d1.grad(L1, [va, vb])\n",
    "    return dL1_da, dL1_db\n",
    "\n",
    "\n",
    "dva, dvb = run_batched_gradients(d3, d2, d1)\n",
    "print(\"va\", va)\n",
    "print(\"vb\", vb)\n",
    "print(\"dva\", dva)\n",
    "print(\"dvb\", dvb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66d013e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "To see that we have done this correctly, we could run the corresponding JAX:\n",
    "\n",
    "```\n",
    "from jax import grad, vmap\n",
    "import jax.numpy as np\n",
    "\n",
    "def simple(a, b):\n",
    "  t = a + b\n",
    "  return t * b\n",
    "\n",
    "def L0(a, b):\n",
    "  return np.sum(simple(a, b))\n",
    "\n",
    "def L1(a, b):\n",
    "  dL0_da, dL0_db = vmap(grad(L0, argnums=(0,1)), in_axes=0)(a, b)\n",
    "  return (dL0_da * dL0_da + dL0_db * dL0_db).sum()\n",
    "\n",
    "va = np.asarray([[0.4556, 0.6323, 0.3489, 0.4017],\n",
    "        [0.0223, 0.1689, 0.2939, 0.5185]])\n",
    "vb = np.asarray([[0.6977, 0.8000, 0.1610, 0.2823],\n",
    "        [0.6816, 0.9152, 0.3971, 0.8742]])\n",
    "dva, dvb = grad(L1, argnums=(0,1))(va, vb)\n",
    "print(\"dva\", dva)\n",
    "print(\"dvb\", dvb)\n",
    "```\n",
    "\n",
    "Looking over the output, the tapes look similar, but we can see that the sizes\n",
    "and the arguments of the operations in question differ (after all, Autograd3 is\n",
    "on the inside of the vmap, while Autograd1 is outside).  But it is still very\n",
    "similar: we could imagine simply varying the dispatcher we use to process backwards\n",
    "depending on when we are executing the tape.  In fact, this is exactly what an\n",
    "initial, non-functorch implementation of PyTorch did to support per-sample\n",
    "gradients.\n",
    "\n",
    "Exercise: modify Autograd.grad to accept a dispatcher, and use that dispatcher\n",
    "instead of self.backward_inner() when running propagator functions.  Then, rewrite\n",
    "the above example so that it only has one level of Autograd:\n",
    "Batched(Autograd(Torch(), create_graph=True)) and show you still get the same\n",
    "result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2706fca7",
   "metadata": {},
   "source": [
    "OK, so all of this dispatcher business is all nice and explicit, but\n",
    "that's not what JAX/functorch's interface looks like.  How do we\n",
    "bridge the gap?  Our first problem is heving to explicitly thread\n",
    "our Dispatcher object everywhere.  In functorch, we instead implicitly\n",
    "have a \"current mode\" which changes when you enter a grad() or vmap()\n",
    "function.  So let's maintain global current dispatcher, and a way to\n",
    "change what the current dispatcher is.  You can think of this as a\n",
    "singly-linked stack of dispatchers which we push and pop dispatchers\n",
    "onto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "c0d9f0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISPATCHER = Torch()\n",
    "\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def dispatcher(d):\n",
    "    global DISPATCHER\n",
    "    old_d = DISPATCHER\n",
    "    DISPATCHER = d\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        DISPATCHER = old_d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f9d0e9",
   "metadata": {},
   "source": [
    "A dispatcher mode, however, is not enough.  Remember that in our\n",
    "implementation of Batched, we blindly assumed that all tensors were\n",
    "batched, even if this did not necessarily make sense.  If I have\n",
    "`vmap(lambda bx: bx + y)(x)`, with `x: (B,X)` and `y: (X,)`, the\n",
    "underlying operation should broadcast y to `(B,X)` and then do the\n",
    "addition with x (bx advertises that it has size `(X,)` inside of the\n",
    "vmap'd lambda).  To know this should happen, it is necessary for\n",
    "us to know that y is not a batched tensor, but x is a batched tensor.\n",
    "We'll resolve this with a wrapper class called FuncTensor, which\n",
    "records both the underlying Tensor, as well as the Dispatcher which\n",
    "this tensor is associated with.  In the above example, `bx.dispatcher`\n",
    "might be `Batched(Torch())`, whereas `x.dispatcher` is `Torch()`.\n",
    "\n",
    "So our general strategy is as follows:\n",
    "  1. Every tensor is associated with a dispatcher\n",
    "  2. You can lift tensors to dispatchers which wrap them (which can\n",
    "     trigger some operations, like expand for Batched); this is\n",
    "     implemented by `dispatcher_wraps`\n",
    "  3. To perform an operation between to tensors, lift them so that\n",
    "     they all have the same dispatcher, then do the operation on\n",
    "     that dispatcher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "c80652b4",
   "metadata": {
    "lines_to_end_of_cell_marker": 2,
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# A dispatcher d1 wraps another dispatcher d2 if d2 is an ancestor of\n",
    "# d1 in the tree structure.  We've defined this relation to be\n",
    "# reflexive, in the same way issubclass(A, A) == True.\n",
    "def dispatcher_wraps(d1, d2):\n",
    "    # Treat this as a reflexive relation\n",
    "    if d1 is d2:\n",
    "        return True\n",
    "    while d1.inner is not None:\n",
    "        d1 = d1.inner\n",
    "        if d1 is d2:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# Given a list of arguments, lift them all up to a common dispatcher\n",
    "# level, returning that dispatcher as well as the lifted arguments.\n",
    "# Note that the global current DISPATCHER is also accounted for!\n",
    "# In autodidax, this is `find_top_trace`.\n",
    "def lift_and_unwrap_args(*args):\n",
    "    outermost = DISPATCHER\n",
    "    for a in args:\n",
    "        if dispatcher_wraps(outermost, a.dispatcher):\n",
    "            pass\n",
    "        elif dispatcher_wraps(a.dispatcher, outermost):\n",
    "            # You can make this case an error as well if you don't\n",
    "            # want to support non-lexical functorch tensors\n",
    "            outermost = a.dispatcher\n",
    "        else:\n",
    "            raise TypeError(\"incompatible dispatcher trees\")\n",
    "    return (outermost,) + tuple(a.lift(outermost).tensor for a in args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ad2cfe",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "The actual implementation of the wrapper tensor which tracks the\n",
    "Dispatcher for a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "454659ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class FuncTensor:\n",
    "    tensor: Tensor\n",
    "    dispatcher: Dispatcher\n",
    "\n",
    "    # Lift a FuncTensor to an outer dispatcher\n",
    "    def lift(self, d):\n",
    "        # You can only lift to a dispatcher which wraps the dispatcher\n",
    "        # this FuncTensor is associated with (not vice versa, or between\n",
    "        # unrelated FuncTensors).\n",
    "        assert dispatcher_wraps(d, self.dispatcher)\n",
    "        return FuncTensor(d.lift(self.tensor, self.dispatcher), d)\n",
    "\n",
    "    # The general strategy for any operation performed on a tensor, we\n",
    "    # lift all the arguments so that they live on the same dispatcher\n",
    "    # level, and then perform the operation on that dispatcher.  The\n",
    "    # resulting tensor is tagged at whatever dispatcher we had run the\n",
    "    # tensor on.\n",
    "    def __mul__(self, other):\n",
    "        d, self, other = lift_and_unwrap_args(self, other)\n",
    "        return FuncTensor(d.mul(self, other), d)\n",
    "\n",
    "    def __add__(self, other):\n",
    "        d, self, other = lift_and_unwrap_args(self, other)\n",
    "        return FuncTensor(d.add(self, other), d)\n",
    "\n",
    "    def sum(self, dim=None, name=None):\n",
    "        d, self = lift_and_unwrap_args(self)\n",
    "        return FuncTensor(d.sum(self, dim, name=name), d)\n",
    "\n",
    "    def expand(self, sizes):\n",
    "        d, self = lift_and_unwrap_args(self)\n",
    "        return FuncTensor(d.expand(self, sizes), d)\n",
    "\n",
    "    def unsqueeze(self, dim):\n",
    "        d, self = lift_and_unwrap_args(self)\n",
    "        return FuncTensor(d.unsqueeze(self, dim), d)\n",
    "\n",
    "    def squeeze(self, dim):\n",
    "        d, self = lift_and_unwrap_args(self)\n",
    "        return FuncTensor(d.squeeze(self, dim), d)\n",
    "\n",
    "    def size(self):\n",
    "        d, self = lift_and_unwrap_args(self)\n",
    "        return d.size(self)\n",
    "\n",
    "    def dim(self):\n",
    "        d, self = lift_and_unwrap_args(self)\n",
    "        return d.size(self)\n",
    "\n",
    "    # Factory functions like ones do not have any Tensor arguments,\n",
    "    # so they rely solely on the ambient DISPATCHER to determine\n",
    "    # what their semantics should be\n",
    "    @staticmethod\n",
    "    def ones(size):\n",
    "        d = lift_and_unwrap_args()\n",
    "        return d.ones(size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6063992f",
   "metadata": {},
   "source": [
    "Now we are ready to implement grad.  First, we need some helper\n",
    "functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "0926b942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When we are done doing a vmap/grad, we need to take the results and\n",
    "# lower them back to a lower dispatcher on the stack (this is always\n",
    "# a no-op, in particular, in the vmap case, when we exit vmap the user\n",
    "# gets to see the batched dimension again.)\n",
    "def unlift(t, d):\n",
    "    if isinstance(t, list):\n",
    "        return [unlift(x, d) for x in t]\n",
    "    elif isinstance(t, tuple):\n",
    "        return tuple(unlift(x, d) for x in t)\n",
    "    else:\n",
    "        if t.dispatcher is d:\n",
    "            return t\n",
    "        return unlift(FuncTensor(t.tensor, t.dispatcher.inner), d)\n",
    "\n",
    "\n",
    "# This lets us easily pick out arguments as specified by argnums\n",
    "def filter_argnums(args, argnums):\n",
    "    if isinstance(argnums, int):\n",
    "        return (args[argnums],)\n",
    "    else:\n",
    "        return tuple(args[i] for i in argnums)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc16dc6",
   "metadata": {},
   "source": [
    "Now grad and vmap!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "8778b72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For simplicity, these functions only take tuples, not pytrees\n",
    "def grad(f, argnums=0):\n",
    "    @functools.wraps(f)\n",
    "    def wrapped_f(*args):\n",
    "        # We first lift and unwrap all of the arguments which we want\n",
    "        # to pass into the function\n",
    "        old_d, *args = lift_and_unwrap_args(*args)\n",
    "        d = Autograd(old_d)\n",
    "        with dispatcher(d):\n",
    "            # We pass in the functions at the new Autograd level (they\n",
    "            # were lifted to old_d, and lifting to d is a noop)\n",
    "            L = f(*(FuncTensor(a, d) for a in args))\n",
    "            assert L.dispatcher is d\n",
    "            # Run the autograd pass, getting the grads for the inputs\n",
    "            # as specified by argnums\n",
    "            grads = d.grad(L.tensor, filter_argnums(args, argnums))\n",
    "            # Finally, construct the grads at the lower level and return\n",
    "            # them\n",
    "            return [FuncTensor(r, old_d) for r in grads]\n",
    "\n",
    "    return wrapped_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "0134b6c8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def vmap(f):\n",
    "    @functools.wraps(f)\n",
    "    def wrapped_f(*args):\n",
    "        # cannot vmap over no arguments as this function uses the\n",
    "        # arguments to determine how large the batch dimension is\n",
    "        # (hypothetically, you could explicitly pass in the batch\n",
    "        # size, and then use this to control factory functions;\n",
    "        # JAX doesn't seem to have a knob to do this)\n",
    "        assert args\n",
    "        old_d, *args = lift_and_unwrap_args(*args)\n",
    "        d = Batched(old_d, length=old_d.size(args[0])[0])\n",
    "        for a in args:\n",
    "            assert old_d.size(a)[0] == d.length\n",
    "        with dispatcher(d):\n",
    "            # Rewrap all the arguments as batched tensors, then\n",
    "            # unwrap any batched tensors that escape\n",
    "            return unlift(f(*(FuncTensor(a, d) for a in args)), old_d)\n",
    "\n",
    "    return wrapped_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f676aa0a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Small test: we want to make sure that we can run multiple layers of vmap and get\n",
    "# the right behavior\n",
    "x = FuncTensor(label(torch.randn(3, 4, 5)), DISPATCHER)\n",
    "ret = vmap(vmap(lambda a: a.unsqueeze(0)))(x)\n",
    "assert ret.size() == (3, 4, 1, 5)\n",
    "assert torch.allclose(ret.tensor, x.unsqueeze(2).tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be60b80",
   "metadata": {},
   "source": [
    "We should see a tensor of size (3, 4, 1, 5) because there's two layers of vmap\n",
    "it's the equivalent of unsqueeze(0) when the tensor is (5,) and then adding (3, 4) back on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159d3937",
   "metadata": {},
   "source": [
    "Now we can rerun our example using the high level grad/vmap functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "2a24e765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autograd v118: (2, 4) = v79 + v80\n",
      "Autograd v118: (4,) = v79 + v80\n",
      "Autograd v119: (2, 4) = v118 * v80\n",
      "Autograd v119: (4,) = v118 * v80\n",
      "Autograd v120: (2,) = v119.sum(dim=(1,))\n",
      "Autograd v120: () = v119.sum(dim=None)\n",
      "-- Autograd dv120 -------\n",
      "Autograd v122: (2, 1) = v121.unsqueeze(dim=1)\n",
      "Autograd v123: (2, 4) = v122.expand((2, 4))\n",
      "Autograd v124: (2, 4) = v123 * v80\n",
      "Autograd v125: (2, 4) = v123 * v118\n",
      "Autograd v126: (2, 4) = v125 + v124\n",
      "------------------------\n",
      "Autograd v127: (2, 4) = v124 * v124\n",
      "Autograd v128: (2, 4) = v126 * v126\n",
      "Autograd v129: (2, 4) = v127 + v128\n",
      "Autograd v130: () = v129.sum(dim=None)\n",
      "-- Autograd dv130 -------\n",
      "------------------------\n",
      "dva FuncTensor(tensor=tensor([[3.7019, 4.4647, 1.3419, 1.9325],\n",
      "        [2.7711, 3.9985, 2.1762, 4.5337]]), dispatcher=<__main__.Torch object at 0x116031640>)\n",
      "dvb FuncTensor(tensor=tensor([[ 8.7992, 10.5293,  3.0059,  4.4296],\n",
      "        [ 6.9054,  9.8274,  5.1466, 10.8156]]), dispatcher=<__main__.Torch object at 0x116031640>)\n"
     ]
    }
   ],
   "source": [
    "def simple(a, b):\n",
    "    t = a + b\n",
    "    return t * b\n",
    "\n",
    "\n",
    "def L0(a, b):\n",
    "    return simple(a, b).sum()\n",
    "\n",
    "\n",
    "def L1(a, b):\n",
    "    dL0_da, dL0_db = vmap(grad(L0, argnums=(0, 1)))(a, b)\n",
    "    return (dL0_da * dL0_da + dL0_db * dL0_db).sum()\n",
    "\n",
    "\n",
    "fva = FuncTensor(va, DISPATCHER)\n",
    "fvb = FuncTensor(vb, DISPATCHER)\n",
    "dva, dvb = grad(L1, argnums=(0, 1))(fva, fvb)\n",
    "print(\"dva\", dva)\n",
    "print(\"dvb\", dvb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268470a8",
   "metadata": {},
   "source": [
    "Because FuncTensors are associated with the ambient dispatcher they\n",
    "were created from, they are also allowed to escape from the context in\n",
    "which they were defined, allowing for non-lexical, imperative\n",
    "transform API.  For example, batching over module parameters is\n",
    "problematic today, but all we need to do is tweak the FuncTensor's\n",
    "dispatchers appropriately and everything works out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "29d937c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expect tensor([[ 1.5606, -1.8643,  0.4406],\n",
      "        [ 1.0271,  0.9861,  0.5888]])\n",
      "output tensor([[ 1.5606, -1.8643,  0.4406],\n",
      "        [ 1.0271,  0.9861,  0.5888]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "PlainTensor = lambda t: FuncTensor(torch.randn(N), DISPATCHER)\n",
    "BatchedTensor = lambda t: FuncTensor(t, Batched(DISPATCHER, length=B))\n",
    "\n",
    "\n",
    "class ScaleBiasModule:\n",
    "    weight: FuncTensor\n",
    "    bias: FuncTensor\n",
    "\n",
    "    def __init__(self, N):\n",
    "        self.weight = PlainTensor(torch.randn(N))\n",
    "        self.bias = PlainTensor(torch.randn(N))\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.weight * input + self.bias\n",
    "\n",
    "\n",
    "B = 2\n",
    "N = 3\n",
    "m = ScaleBiasModule(N)\n",
    "# Ensemble weights only; input is not batched\n",
    "m.weight = BatchedTensor(torch.randn(B, N))\n",
    "input = PlainTensor(torch.randn(N))\n",
    "output = m.forward(input)\n",
    "print(\n",
    "    \"expect\", input.tensor.unsqueeze(0) * m.weight.tensor + m.bias.tensor.unsqueeze(0)\n",
    ")\n",
    "print(\"output\", output.tensor)\n",
    "\n",
    "# Autodidax decoder ring\n",
    "#\n",
    "# Tracer = FuncTensor\n",
    "#   can be subclassed for storing extra data (JVPTracer, BatchTracer)\n",
    "# AbstractValue = this represents the \"dtype\" and the \"sizes\", etc (stored on Tracer)\n",
    "#   It's a type! But it also has sizes (like a meta tensor)\n",
    "#   ShapedArray/ConcreteArray (distinguish between device and meta\n",
    "#   tensor) ~> going to bind\n",
    "#   ...not really Dispatcher; represents the dtype/size stuff\n",
    "# MainTrace = Dispatcher (as stored in DISPATCHER stack) (why do they\n",
    "#   also need Trace? Weird.  as seen in jvp_v1 they first new_main\n",
    "#   and then wrap it in JVPTrace)\n",
    "# Trace = Dispatcher (the thing that gets subclassed to have\n",
    "#   implementations, e.g., EvalTrace, JVPTrace)\n",
    "#   pure = take a constant and make the Tracer for it\n",
    "#   lift = take an inner Tracer and make it this level\n",
    "#\n",
    "# new_main ~> dispatcher, gives you the Dispatcher/MainTrace\n",
    "# get_aval(t) ~> getting the dtype/size metadata stuff\n",
    "# bind ~> inlined in FuncTensor methods, including lift_and_unwrap_args\n",
    "# full_lower ~> not implemented\n",
    "# full_raise ~> lift\n",
    "# find_top_trace ~> lift_and_unwrap_args\n",
    "#\n",
    "# dynamic_trace ~> the thing that pushes jit to the bottom\n",
    "#\n",
    "# jitting gives you an xla_call_p, which STORES the jaxpr\n",
    "#   transforms look into the jaxpr and retrace it with the transform\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04331cf8",
   "metadata": {},
   "source": [
    "Higher order operators in simple_functorch!\n",
    "\n",
    "Problem: users want to define functions with custom forward and backward\n",
    "passes. These functions call PyTorch operations. When we vmap over such a\n",
    "function, we would like for the backward pass to be preserved.\n",
    "\n",
    "Why is this difficult? In PyTorch today, vmap over an autograd.Function\n",
    "effectively runs vmap on the forward pass of the autograd.Function.\n",
    "Meanwhile, autograd records the transformed operations for backward, instead\n",
    "of the custom backward pass we specified in the autograd.Function!\n",
    "\n",
    "Solution: We're going to introduce a `custom_vjp` primitive that accepts\n",
    "functions and varargs Tensor arguments and demonstrate that it resolves\n",
    "the problem.\n",
    "\n",
    "custom_vjp(fwd_fn, bwd_fn, *args) takes in two functions as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48d3930",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For our custom function, we want f(x) = x * x, but we install a custom\n",
    "# backwards pass that computes 32 * x (instead of 2 * x) so we can tell\n",
    "# if custom_vjp is working.\n",
    "\n",
    "d = Autograd(Torch())\n",
    "\n",
    "a = label(torch.rand(4))\n",
    "va = label(torch.rand(2, 4))\n",
    "\n",
    "\n",
    "def f_fwd(dispatcher, x):\n",
    "    # Our convention is that f_fwd returns (outputs, \"saved\")\n",
    "    return dispatcher.mul(x, x), x\n",
    "\n",
    "\n",
    "# Our convention is that f_bwd accepts (dispatcher, gradOutputs, \"saved\")\n",
    "def f_bwd(dispatcher, gradOutputs, x):\n",
    "    (gO,) = gradOutputs\n",
    "    # Should be gO * 2 * x, but we're gonna do gO * 32 * x to demonstrate things\n",
    "    return [\n",
    "        dispatcher.mul(dispatcher.mul(gO, x), label(torch.tensor(32.0), \"thirty_two\"))\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591b1ff2",
   "metadata": {},
   "source": [
    "Okay, now let's try it out!\n",
    "\n",
    "The implementaton of custom_vjp is:\n",
    "- call custom_vjp on the inner dispatcher\n",
    "- save f_bwd as a lambda onto the gradient tape\n",
    "(See earlier in this colab for more details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801089ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# A single layer of autograd\n",
    "def run_grad(d):\n",
    "    # Here's how to invoke custom_vjp.\n",
    "    r, _ = d.custom_vjp(f_fwd, f_bwd, a)\n",
    "    L3 = d.sum(r, name=\"L3\")\n",
    "    (dL3_a,) = d.grad(L3, [a])\n",
    "\n",
    "    # Check that the gradients are indeed 32 * a\n",
    "    assert torch.allclose(dL3_a, 32 * a)\n",
    "\n",
    "\n",
    "run_grad(d)\n",
    "\n",
    "# Multiple layers of autograd\n",
    "def run_gradgrad(d2, d1):\n",
    "    r, _ = d2.custom_vjp(f_fwd, f_bwd, a)\n",
    "    L4 = d2.sum(r, name=\"L4\")\n",
    "    (dL4_a,) = d2.grad(L4, [a])\n",
    "\n",
    "    # Evidence that d2 respected the custom_vjp's f_bwd\n",
    "    assert torch.allclose(dL4_a, 32 * a)\n",
    "\n",
    "    assert hasattr(dL4_a, \"t_name\")\n",
    "    dL4_a_sum = d1.sum(dL4_a, name=\"dL4_a_sum\")\n",
    "    (ddL4_a_a,) = d1.grad(dL4_a_sum, [a])\n",
    "\n",
    "    # Evidence that d1 respected the custom_vjp's f_bwd\n",
    "    assert torch.allclose(ddL4_a_a, torch.ones_like(a) * 32)\n",
    "\n",
    "\n",
    "d1 = Autograd(Logger(Torch(), name=\"Torch\"), name=\"Autograd1\", create_graph=False)\n",
    "d2 = Autograd(d1, name=\"Autograd2\", create_graph=False)\n",
    "run_gradgrad(d2, d1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872ab2b3",
   "metadata": {},
   "source": [
    "And now, let's try that again, with grad(lambda x: vmap(f)(x).sum()).\n",
    "The goal of custom_vjp is to make it so that vmap(custom_vjp) still\n",
    "preserves the backward semantics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c1b853",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def f_fwd(d, x):\n",
    "    return d.mul(x, x), x\n",
    "\n",
    "\n",
    "def f_bwd(d, gradOutputs, x):\n",
    "    (gO,) = gradOutputs\n",
    "    # Should be gO * 2 * x, but we're gonna do gO * 32 * x to prove a point\n",
    "    return [d.mul(d.mul(gO, x), label(torch.ones_like(x) * 32.0, \"thirty_two\"))]\n",
    "\n",
    "\n",
    "d1 = Autograd(Torch(), name=\"Autograd1\", create_graph=False)\n",
    "d2 = Batched(d1, length=2, name=\"Batched2\")\n",
    "\n",
    "\n",
    "def run_gradvmap(d2: \"Batched\", d1: \"Autograd\"):\n",
    "    r, _ = d2.custom_vjp(f_fwd, f_bwd, va)\n",
    "    L99 = d1.sum(r, name=\"L99\")\n",
    "    (dL99_a,) = d1.grad(L99, [va])\n",
    "\n",
    "    # As you can see, d1.grad still calls f_bwd.\n",
    "    # The way we got this to work is that Batched.custom_vjp\n",
    "    # calls custom_vjp on its inner dispatcher.\n",
    "    # Scroll up to the implementation of Batched for more details.\n",
    "    assert torch.allclose(dL99_a, 32 * va)\n",
    "\n",
    "\n",
    "run_gradvmap(d2, d1)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
