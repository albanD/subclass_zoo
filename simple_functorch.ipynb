{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cb2ffc9",
   "metadata": {},
   "source": [
    "This notebook walks through a self-contained implementation of\n",
    "functorch, including support for both vjp and vmap combinators (using\n",
    "PyTorch only to implement primitive tensor operations).  It follows\n",
    "the tradition of\n",
    "[Autodidax](https://jax.readthedocs.io/en/latest/autodidax.html) (a\n",
    "pedagogical reimplementation of JAX, the library functorch is inspired\n",
    "by) and [Simple\n",
    "Autograd](https://colab.research.google.com/drive/1VpeE6UvEPRz9HmsHh1KS0XxXjYu533EC?usp=sharing)\n",
    "(Zachary Devito's pedagogical reimplementation of autograd, which the\n",
    "autograd system in this notebook is based off of.) You can [open this\n",
    "file in\n",
    "Colab](https://colab.research.google.com/github/albanD/subclass_zoo/blob/main/simple_functorch.ipynb)\n",
    "and play around with the examples.\n",
    "\n",
    "As a simplified implementation of functorch, this notebook also makes\n",
    "it easier to investigate some more subtle aspects of how PyTorch's\n",
    "native autograd system interacts with composable transforms.  In\n",
    "particular, we will see that PyTorch's native implementation of double\n",
    "backwards (which shares the same tape through multiple levels of\n",
    "differentiation) differs from functorch's nested grad implementation\n",
    "(which maintains a separate tape per level)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed150b0",
   "metadata": {},
   "source": [
    "To get started, we replicate some of the data structures and helper functions\n",
    "from Simple Autograd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba464220",
   "metadata": {
    "lines_to_end_of_cell_marker": 2
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from typing import List, NamedTuple, Callable, Dict, Optional\n",
    "\n",
    "\n",
    "class TapeEntry(NamedTuple):\n",
    "    # names of the inputs to the original computation\n",
    "    inputs: List[str]\n",
    "    # names of the outputs of the original computation\n",
    "    outputs: List[str]\n",
    "    # apply chain rule\n",
    "    propagate: Callable[[List[Tensor]], List[Tensor]]\n",
    "\n",
    "\n",
    "_name = 0\n",
    "\n",
    "\n",
    "def fresh_name() -> str:\n",
    "    \"\"\"create a new unique name for a variable: v0, v1, v2\"\"\"\n",
    "    global _name\n",
    "    r = f\"v{_name}\"\n",
    "    _name += 1\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af3ca0b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "This is a little helper function for converting the dim argument in\n",
    "sum into an explicit list of dimensions that will be reduced over.\n",
    "It takes the dim of the tensor we are summing over and the dim\n",
    "argument itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "836a1635",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def sum_dims(*, input_dim, dim):\n",
    "    if dim is None:\n",
    "        return tuple(range(0, input_dim))\n",
    "    elif isinstance(dim, int):\n",
    "        return (dim,)\n",
    "    else:\n",
    "        return tuple(sorted(dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f74fb48",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In Simple Autograd, we provided a Variable wrapper class which\n",
    "provided a traditional Tensor style interface for our objects; in\n",
    "functorch proper, objects are repeatedly wrapped in this way to\n",
    "implement multipler layers of transformations.\n",
    "\n",
    "In my opinion, this sort of wrapper makes it more difficult to\n",
    "understand the flow of logic.  So in Simple Functorch, we take a\n",
    "different approach: we won't make use of a wrapper class at all,\n",
    "instead showing how to add it in the end as syntax sugar on top of our\n",
    "system.\n",
    "\n",
    "For debuggability purposes, however, it is nice to have a way to\n",
    "identify variables by a human readable name.  We'll do this by setting\n",
    "a t_name attribute on PyTorch tensors whenever we allocate a new\n",
    "tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9d17aef",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def label(t: Tensor, name: str = None):\n",
    "    if not hasattr(t, \"t_name\"):\n",
    "        t.t_name = name or fresh_name()\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd046576",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "So if we aren't going to have a wrapper around each tensor, how will\n",
    "we actually implement our logic?  We will organize our various layers\n",
    "of transformations as separate Dispatcher objects, which define\n",
    "methods for performing operations on tensors, but are not Tensors\n",
    "themselves.  For example, instead of defining Tensor.add(Tensor), we\n",
    "will define Dispatcher.add(Tensor, Tensor).  If you are familiar with\n",
    "historical ATen, in the original implementation of ATen, these\n",
    "correspond to the CPUType/CUDAType/VariableType objects from that\n",
    "implementation (this was replaced with the modern dispatcher as the\n",
    "original vtable-based implementation did not support adding custom\n",
    "operators.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59a69d9d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class Dispatcher:\n",
    "    def mul(self, lhs, rhs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def add(self, lhs, rhs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # Sum has been generalized to take an optional dim argument, which we\n",
    "    # will need for Batched tensors\n",
    "    def sum(self, input, dim=None, name=None):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def expand(self, input, sizes):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # For closure under Batched tensors, we need these operations...\n",
    "    def unsqueeze(self, input, dim):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def squeeze(self, input, dim):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # ...and we also need to overload the meaning of size/ones to\n",
    "    # hide/reinsert batch dimensions\n",
    "    def size(self, input):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def ones(self, size):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # For convenience, we provide dim, which just returns the length of\n",
    "    # the sizes\n",
    "    def dim(self, input):\n",
    "        return len(self.size(input))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e21984",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "To start with, we can implement a backend dispatcher layer Torch,\n",
    "which just forwards the operator calls to our underlying library PyTorch\n",
    "(and ensures that all the allocated tensors are labeled with variable).  You could\n",
    "also imagine replacing this with a Numpy backend or even a pure Python\n",
    "variant (although this file is not currently setup to do so.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27101073",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class Torch(Dispatcher):\n",
    "    def mul(self, lhs, rhs):\n",
    "        return label(torch.mul(lhs, rhs))\n",
    "\n",
    "    def add(self, lhs, rhs):\n",
    "        return label(torch.add(lhs, rhs))\n",
    "\n",
    "    def sum(self, input, dim=None, name=None):\n",
    "        if dim is None:\n",
    "            return label(torch.sum(input), name)\n",
    "        else:\n",
    "            return label(torch.sum(input, dim), name)\n",
    "\n",
    "    def expand(self, input, sizes):\n",
    "        return label(input.expand(sizes))\n",
    "\n",
    "    def unsqueeze(self, input, dim):\n",
    "        return label(torch.unsqueeze(input, dim))\n",
    "\n",
    "    def squeeze(self, input, dim):\n",
    "        return label(torch.squeeze(input, dim))\n",
    "\n",
    "    def size(self, input):\n",
    "        # Return size a tuple for marginally more compact printing\n",
    "        return tuple(input.size())\n",
    "\n",
    "    def ones(self, size):\n",
    "        return label(torch.ones(size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e428f835",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Dispatcher layers are composable via object composition: we can\n",
    "imagine a stack of dispatchers, each one calling into the next.\n",
    "For example, the Logger dispatcher simply prints out what operation\n",
    "was called on it, and then forwards on the operation to the inner\n",
    "dispatcher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1500269c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger(Dispatcher):\n",
    "    def __init__(self, inner, *, name):\n",
    "        self.inner = inner\n",
    "        self.name = f\"  {name}\"\n",
    "\n",
    "    def size(self, input):\n",
    "        # don't log size calls\n",
    "        return self.inner.size(input)\n",
    "\n",
    "    def ones(self, size):\n",
    "        r = self.inner.ones(size)\n",
    "        print(f\"{self.name} {r.t_name}: {self.size(r)} = ones({size})\")\n",
    "        return r\n",
    "\n",
    "    def mul(self, lhs, rhs):\n",
    "        r = self.inner.mul(lhs, rhs)\n",
    "        if isinstance(rhs, float):\n",
    "            print(f\"{self.name} {r.t_name}: {self.size(r)} = {lhs.t_name} * {rhs}\")\n",
    "        else:\n",
    "            print(\n",
    "                f\"{self.name} {r.t_name}: {self.size(r)} = {lhs.t_name} * {rhs.t_name}\"\n",
    "            )\n",
    "        return r\n",
    "\n",
    "    def add(self, lhs, rhs):\n",
    "        r = self.inner.add(lhs, rhs)\n",
    "        print(f\"{self.name} {r.t_name}: {self.size(r)} = {lhs.t_name} + {rhs.t_name}\")\n",
    "        return r\n",
    "\n",
    "    def sum(self, input, dim=None, name=None):\n",
    "        r = self.inner.sum(input, dim=dim, name=name)\n",
    "        print(f\"{self.name} {r.t_name}: {self.size(r)} = {input.t_name}.sum(dim={dim})\")\n",
    "        return r\n",
    "\n",
    "    def unsqueeze(self, input, dim):\n",
    "        r = self.inner.unsqueeze(input, dim)\n",
    "        print(\n",
    "            f\"{self.name} {r.t_name}: {self.size(r)} = {input.t_name}.unsqueeze({dim})\"\n",
    "        )\n",
    "        return r\n",
    "\n",
    "    def squeeze(self, input, dim):\n",
    "        r = self.inner.squeeze(input, dim)\n",
    "        print(f\"{self.name} {r.t_name}: {self.size(r)} = {input.t_name}.squeeze({dim})\")\n",
    "        return r\n",
    "\n",
    "    def expand(self, input, sizes):\n",
    "        r = self.inner.expand(input, sizes)\n",
    "        print(\n",
    "            f\"{self.name} {r.t_name}: {self.size(r)} = {input.t_name}.expand({sizes})\"\n",
    "        )\n",
    "        return r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e323b552",
   "metadata": {},
   "source": [
    "Here is a simple example of using Logger and Torch together.  Whenever\n",
    "we make calls to operations, we must do so via the Dispatcher object.\n",
    "We will explicitly write out all of these calls before we add wrapper\n",
    "class sugaring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26b4c5e5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "d = Logger(Torch(), name=\"Torch\")\n",
    "print(d.add(d.ones(2), d.ones(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcd422b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "With the Dispatcher structure in hand, we are now in a good place to\n",
    "port the autograd implementation from Simple Autograd into our new\n",
    "framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54293b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autograd(Dispatcher):\n",
    "    # create_graph here corresponds to the create_graph kwarg in traditional\n",
    "    # PyTorch, which controls whether or not the graph of the derivative\n",
    "    # will be constructed, allowing computing higher order derivatives.\n",
    "    # We will see that although create_graph=True allows Autograd to directly\n",
    "    # support higher order derivatives, layering an Autograd to another\n",
    "    # Autograd will also allow higher order derivatives.\n",
    "    def __init__(self, inner, *, name=\"Autograd\", create_graph: bool = False):\n",
    "        self.inner = inner\n",
    "        self.gradient_tape = []\n",
    "        self.name = name\n",
    "        self.create_graph = create_graph\n",
    "\n",
    "    # create_graph controls where add/mul/etc calls from the backwards\n",
    "    # propagators go: if you create_graph, they recursively call back to\n",
    "    # the current Autograd dispatcher; otherwise they move on to the inner\n",
    "    # layer.\n",
    "    def backward_inner(self):\n",
    "        if self.create_graph:\n",
    "            return self\n",
    "        else:\n",
    "            return self.inner\n",
    "\n",
    "    def mul(self, lhs, rhs):\n",
    "        if isinstance(rhs, float) and rhs == 1.0:\n",
    "            # peephole optimization\n",
    "            return lhs\n",
    "\n",
    "        # define forward\n",
    "        # first, run the operation in the inner layer to get the initial\n",
    "        # result\n",
    "        r = self.inner.mul(lhs, rhs)\n",
    "        # We directly implement printing here as it indicates whether or not\n",
    "        # this operation was saved to the tape or not\n",
    "        print(f\"{self.name} {r.t_name}: {self.size(r)} = {lhs.t_name} * {rhs.t_name}\")\n",
    "\n",
    "        # record what the inputs and outputs of the op were\n",
    "        inputs = [lhs.t_name, rhs.t_name]\n",
    "        outputs = [r.t_name]\n",
    "\n",
    "        # define backprop\n",
    "        def propagate(dL_doutputs: List[Tensor]):\n",
    "            (dL_dr,) = dL_doutputs\n",
    "\n",
    "            dr_dlhs = rhs  # partial derivative of r = lhs*rhs\n",
    "            dr_drhs = lhs  # partial derivative of r = lhs*rhs\n",
    "\n",
    "            # chain rule propagation from outputs to inputs of multiply.\n",
    "            # Notice that the propagation rule may itself call\n",
    "            # other operations; depending on create_graph, they may\n",
    "            # either go to self or self.inner; self.backward_inner()\n",
    "            # controls which one we go to.\n",
    "            dL_dlhs = self.backward_inner().mul(dL_dr, dr_dlhs)\n",
    "            dL_drhs = self.backward_inner().mul(dL_dr, dr_drhs)\n",
    "            dL_dinputs = [dL_dlhs, dL_drhs]\n",
    "            return dL_dinputs\n",
    "\n",
    "        # finally, we record the compute we did on the tape\n",
    "        self.gradient_tape.append(\n",
    "            TapeEntry(inputs=inputs, outputs=outputs, propagate=propagate)\n",
    "        )\n",
    "        return r\n",
    "\n",
    "    # The rest of the implementations follow in the same way and can\n",
    "    # be skipped\n",
    "\n",
    "    def add(self, lhs, rhs):\n",
    "        # Add follows a similar pattern to Mul, but it doesn't end up\n",
    "        # capturing any variables.\n",
    "        r = self.inner.add(lhs, rhs)\n",
    "        print(f\"{self.name} {r.t_name}: {self.size(r)} = {lhs.t_name} + {rhs.t_name}\")\n",
    "\n",
    "        def propagate(dL_doutputs: List[Tensor]):\n",
    "            (dL_dr,) = dL_doutputs\n",
    "            dr_dlhs = 1.0\n",
    "            dr_drhs = 1.0\n",
    "            dL_dlhs = self.backward_inner().mul(dL_dr, dr_dlhs)\n",
    "            dL_drhs = self.backward_inner().mul(dL_dr, dr_drhs)\n",
    "            return [dL_dlhs, dL_drhs]\n",
    "\n",
    "        self.gradient_tape.append(\n",
    "            TapeEntry(\n",
    "                inputs=[lhs.t_name, rhs.t_name], outputs=[r.t_name], propagate=propagate\n",
    "            )\n",
    "        )\n",
    "        return r\n",
    "\n",
    "    # Extended to handle dim argument for Batched (later)\n",
    "    def sum(self, input: Tensor, dim=None, name: Optional[str] = None):\n",
    "        r = self.inner.sum(input, dim=dim, name=name)\n",
    "        print(f\"{self.name} {r.t_name}: {self.size(r)} = {input.t_name}.sum(dim={dim})\")\n",
    "\n",
    "        def propagate(dL_doutputs: List[Tensor]):\n",
    "            (dL_dr,) = dL_doutputs\n",
    "            size = self.inner.size(input)\n",
    "            res = dL_dr\n",
    "            # Broadcast over all dimensions that were reduced over\n",
    "            for i in sum_dims(input_dim=self.inner.dim(input), dim=dim):\n",
    "                res = self.backward_inner().unsqueeze(res, i)\n",
    "            return [self.backward_inner().expand(res, size)]\n",
    "\n",
    "        self.gradient_tape.append(\n",
    "            TapeEntry(inputs=[input.t_name], outputs=[r.t_name], propagate=propagate)\n",
    "        )\n",
    "        return r\n",
    "\n",
    "    # Unlike Simple Autograd, this expand requires the input to have\n",
    "    # been unsqueezed before hand.  This lets us avoid having to do\n",
    "    # at::sum_to for the nontrivial case (which is more complicated)\n",
    "    def expand(self, input: Tensor, sizes: List[int]):\n",
    "        assert self.inner.dim(input) == len(sizes)  # only works if dims match\n",
    "        r = self.inner.expand(input, sizes)\n",
    "        print(\n",
    "            f\"{self.name} {r.t_name}: {self.size(r)} = {input.t_name}.expand({sizes})\"\n",
    "        )\n",
    "\n",
    "        def propagate(dL_doutputs: List[Tensor]):\n",
    "            (dL_dr,) = dL_doutputs\n",
    "            input_size = self.inner.size(input)\n",
    "            dims = tuple(\n",
    "                i for i in range(self.inner.dim(input)) if input_size[i] != sizes[i]\n",
    "            )\n",
    "            # We wanted a sum keepdim=True, but I didn't want to force\n",
    "            # everyone to support it so manually unsqueeze\n",
    "            res = self.backward_inner().sum(dL_dr, dims)\n",
    "            for d in dims:\n",
    "                res = self.backward_inner().unsqueeze(res, d)\n",
    "            return [res]\n",
    "\n",
    "        self.gradient_tape.append(\n",
    "            TapeEntry(inputs=[input.t_name], outputs=[r.t_name], propagate=propagate)\n",
    "        )\n",
    "        return r\n",
    "\n",
    "    # Unsqueeze are required for sum backwards, and then squeeze is required\n",
    "    # for closure.  Size needed for batched tensor to modify size.\n",
    "\n",
    "    def size(self, input: Tensor):\n",
    "        return self.inner.size(input)\n",
    "\n",
    "    def squeeze(self, input: Tensor, dim):\n",
    "        r = self.inner.squeeze(input, dim)\n",
    "        print(\n",
    "            f\"{self.name} {r.t_name}: {self.size(r)} = {input.t_name}.squeeze(dim={dim})\"\n",
    "        )\n",
    "\n",
    "        def propagate(dL_doutputs: List[Tensor]):\n",
    "            (dL_dr,) = dL_outputs\n",
    "            return [self.backward_inner().unsqueeze(dL_dr, dim)]\n",
    "\n",
    "        self.gradient_tape.append(\n",
    "            TapeEntry(inputs=[input.t_name], outputs=[r.t_name], propagate=propagate)\n",
    "        )\n",
    "        return r\n",
    "\n",
    "    def unsqueeze(self, input: Tensor, dim):\n",
    "        r = self.inner.unsqueeze(input, dim)\n",
    "        print(\n",
    "            f\"{self.name} {r.t_name}: {self.size(r)} = {input.t_name}.unsqueeze(dim={dim})\"\n",
    "        )\n",
    "\n",
    "        def propagate(dL_doutputs: List[Tensor]):\n",
    "            (dL_dr,) = dL_doutputs\n",
    "            return [self.backward_inner().squeeze(dL_dr, dim)]\n",
    "\n",
    "        self.gradient_tape.append(\n",
    "            TapeEntry(inputs=[input.t_name], outputs=[r.t_name], propagate=propagate)\n",
    "        )\n",
    "        return r\n",
    "\n",
    "    def ones(self, size):\n",
    "        return self.inner.ones(size)\n",
    "\n",
    "    def grad(self, L, desired_results: List[Tensor]) -> List[Tensor]:\n",
    "        # this map holds dL/dX for all values X\n",
    "        dL_d: Dict[str, Tensor] = {}\n",
    "        # It starts by initializing the 'seed' dL/dL, which is 1\n",
    "        # TODO: indirect this via the backend\n",
    "        dL_d[L.t_name] = self.inner.ones(())\n",
    "        print(f\"-- {self.name} d{L.t_name} -------\")\n",
    "\n",
    "        # look up dL_dentries. If a variable is never used to compute the loss,\n",
    "        # we consider its gradient None, see the note below about zeros for more information.\n",
    "        def gather_grad(entries: List[str]):\n",
    "            return [dL_d[entry] if entry in dL_d else None for entry in entries]\n",
    "\n",
    "        # propagate the gradient information backward\n",
    "        for entry in reversed(self.gradient_tape):\n",
    "            dL_doutputs = gather_grad(entry.outputs)\n",
    "            if all(dL_doutput is None for dL_doutput in dL_doutputs):\n",
    "                # optimize for the case where some gradient pathways are zero. See\n",
    "                # The note below for more details.\n",
    "                continue\n",
    "\n",
    "            # perform chain rule propagation specific to each compute\n",
    "            dL_dinputs = entry.propagate(dL_doutputs)\n",
    "\n",
    "            # Accululate the gradient produced for each input.\n",
    "            # Each use of a variable produces some gradient dL_dinput for that\n",
    "            # use. The multivariate chain rule tells us it is safe to sum\n",
    "            # all the contributions together.\n",
    "            for input, dL_dinput in zip(entry.inputs, dL_dinputs):\n",
    "                if input not in dL_d:\n",
    "                    dL_d[input] = dL_dinput\n",
    "                else:\n",
    "                    dL_d[input] = self.backward_inner().add(dL_d[input], dL_dinput)\n",
    "\n",
    "        # print some information to understand the values of each intermediate\n",
    "        # for name, value in dL_d.items():\n",
    "        #     print(f'{self.name} d{L.t_name}_d{name} = {value.t_name}')\n",
    "        print(f\"------------------------\")\n",
    "\n",
    "        return gather_grad(desired.t_name for desired in desired_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901c6b4a",
   "metadata": {},
   "source": [
    "To calculate some simple gradients, we can compose Autograd with\n",
    "Torch and get the result we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b725c318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autograd v2: (4,) = v0 + v1\n",
      "Autograd v3: (4,) = v2 * v1\n",
      "-- Autograd dv3 -------\n",
      "------------------------\n",
      "da tensor([0.3074, 0.6341, 0.4901, 0.8964])\n",
      "db tensor([1.1111, 2.0364, 1.0687, 1.9249])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "a, b = label(torch.rand(4)), label(torch.rand(4))\n",
    "\n",
    "\n",
    "def simple(d, a, b):\n",
    "    t = d.add(a, b)\n",
    "    return d.mul(t, b)\n",
    "\n",
    "\n",
    "d = Autograd(Torch())\n",
    "\n",
    "loss = simple(d, a, b)\n",
    "da, db = d.grad(loss, [a, b])\n",
    "print(\"da\", da)\n",
    "print(\"db\", db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd64990",
   "metadata": {},
   "source": [
    "To compute higher order gradients, we have two options.  First,\n",
    "we can do traditional PyTorch style higher order differentiation\n",
    "with `create_graph=True`, writing the backpropagation computations directly\n",
    "into the tape so they can be further differentiated over.  This is also\n",
    "what the original Simple Autograd implementation does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e1a5342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autograd v10: (4,) = v0 + v1\n",
      "Autograd v11: (4,) = v10 * v1\n",
      "Autograd L0: () = v11.sum(dim=None)\n",
      "-- Autograd dL0 -------\n",
      "Autograd v13: (1,) = v12.unsqueeze(dim=0)\n",
      "Autograd v14: (4,) = v13.expand((4,))\n",
      "Autograd v15: (4,) = v14 * v1\n",
      "Autograd v16: (4,) = v14 * v10\n",
      "Autograd v17: (4,) = v16 + v15\n",
      "------------------------\n",
      "Autograd v18: (4,) = v15 * v15\n",
      "Autograd v19: (4,) = v17 * v17\n",
      "Autograd v20: (4,) = v18 + v19\n",
      "Autograd L1: () = v20.sum(dim=None)\n",
      "-- Autograd dL1 -------\n",
      "Autograd v22: (1,) = v21.unsqueeze(dim=0)\n",
      "Autograd v23: (4,) = v22.expand((4,))\n",
      "Autograd v24: (4,) = v23 * v17\n",
      "Autograd v25: (4,) = v23 * v17\n",
      "Autograd v26: (4,) = v24 + v25\n",
      "Autograd v27: (4,) = v23 * v15\n",
      "Autograd v28: (4,) = v23 * v15\n",
      "Autograd v29: (4,) = v27 + v28\n",
      "Autograd v30: (4,) = v29 + v26\n",
      "Autograd v31: (4,) = v26 * v10\n",
      "Autograd v32: (4,) = v26 * v14\n",
      "Autograd v33: (4,) = v30 * v1\n",
      "Autograd v34: (4,) = v30 * v14\n",
      "Autograd v35: (4,) = v31 + v33\n",
      "Autograd v36: () = v35.sum(dim=(0,))\n",
      "Autograd v37: (1,) = v36.unsqueeze(dim=0)\n",
      "Autograd v38: () = v37.squeeze(dim=0)\n",
      "Autograd v39: (4,) = v34 + v32\n",
      "------------------------\n",
      "da tensor([2.2222, 4.0728, 2.1373, 3.8498])\n",
      "db tensor([5.0593, 9.4137, 5.2548, 9.4926])\n"
     ]
    }
   ],
   "source": [
    "d = Autograd(Torch(), create_graph=True)\n",
    "\n",
    "# I slightly generalized this function so that it works for the next\n",
    "# example; d2 is the dispatcher run on the first grad call, and d1 is\n",
    "# for the second (we'll see why the numbers are inverted shortly).\n",
    "def run_gradients(d2, d1):\n",
    "    # our first loss\n",
    "    L0 = d2.sum(simple(d2, a, b), name=\"L0\")\n",
    "\n",
    "    # compute derivatives of our inputs\n",
    "    dL0_da, dL0_db = d2.grad(L0, [a, b])\n",
    "\n",
    "    # In real code, how would we switch from executing from d2 to d1?\n",
    "    # In functorch, the d2 dispatch calls would happen in the inside of\n",
    "    # a higher-order grad() call; when we exit from this call, all\n",
    "    # of the involved tensors are unwrapped.\n",
    "\n",
    "    # now lets compute the L2 norm of our derivatives\n",
    "    L1 = d1.sum(d1.add(d1.mul(dL0_da, dL0_da), d1.mul(dL0_db, dL0_db)), name=\"L1\")\n",
    "\n",
    "    # and take the gradient of that.\n",
    "    # notice there are two losses involved1.\n",
    "    dL1_da, dL1_db = d1.grad(L1, [a, b])\n",
    "    return dL1_da, dL1_db\n",
    "\n",
    "\n",
    "da, db = run_gradients(d, d)\n",
    "print(\"da\", da)\n",
    "print(\"db\", db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b17d02c",
   "metadata": {},
   "source": [
    "Our second option is to follow functorch's implementation strategy, which\n",
    "is to stack two Autograd dispatchers on top of each other.  Here, it is\n",
    "not necessary to `create_graph=True`, because when the backpropagator forwards\n",
    "to the inner dispatcher, it will record those operations on the tape too.\n",
    "But if you look at the output, you will notice something very interesting:\n",
    "the first portion of the tape is exactly replicated between Autograd1 and\n",
    "Autograd2: we're duplicating the tape in this case!  So PyTorch's default\n",
    "implementation of backwards is more efficient, because it avoids having to\n",
    "record the tape twice (although this doesn't matter too much, because the\n",
    "saved tensors themselves can be shared between the two tapes, so it is just\n",
    "the operator graph that is duplicated.\n",
    "\n",
    "This is our first example of using two dispatchers.  While we are\n",
    "performing the inner grad, we perform our operations on the outer\n",
    "dispatcher `d2`; after we are done with the inner grad we switch to\n",
    "`d1`.  Intuitively, this corresponds from passing out of the inner\n",
    "`grad` call to the outer `grad` call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7c47f5e",
   "metadata": {
    "lines_to_end_of_cell_marker": 0,
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Torch v40: (4,) = v0 + v1\n",
      "Autograd1 v40: (4,) = v0 + v1\n",
      "Autograd2 v40: (4,) = v0 + v1\n",
      "  Torch v41: (4,) = v40 * v1\n",
      "Autograd1 v41: (4,) = v40 * v1\n",
      "Autograd2 v41: (4,) = v40 * v1\n",
      "  Torch L0: () = v41.sum(dim=None)\n",
      "Autograd1 L0: () = v41.sum(dim=None)\n",
      "Autograd2 L0: () = v41.sum(dim=None)\n",
      "  Torch v42: () = ones(())\n",
      "-- Autograd2 dL0 -------\n",
      "  Torch v43: (1,) = v42.unsqueeze(0)\n",
      "Autograd1 v43: (1,) = v42.unsqueeze(dim=0)\n",
      "  Torch v44: (4,) = v43.expand((4,))\n",
      "Autograd1 v44: (4,) = v43.expand((4,))\n",
      "  Torch v45: (4,) = v44 * v1\n",
      "Autograd1 v45: (4,) = v44 * v1\n",
      "  Torch v46: (4,) = v44 * v40\n",
      "Autograd1 v46: (4,) = v44 * v40\n",
      "  Torch v47: (4,) = v46 + v45\n",
      "Autograd1 v47: (4,) = v46 + v45\n",
      "------------------------\n",
      "  Torch v48: (4,) = v45 * v45\n",
      "Autograd1 v48: (4,) = v45 * v45\n",
      "  Torch v49: (4,) = v47 * v47\n",
      "Autograd1 v49: (4,) = v47 * v47\n",
      "  Torch v50: (4,) = v48 + v49\n",
      "Autograd1 v50: (4,) = v48 + v49\n",
      "  Torch L1: () = v50.sum(dim=None)\n",
      "Autograd1 L1: () = v50.sum(dim=None)\n",
      "  Torch v51: () = ones(())\n",
      "-- Autograd1 dL1 -------\n",
      "  Torch v52: (1,) = v51.unsqueeze(0)\n",
      "  Torch v53: (4,) = v52.expand((4,))\n",
      "  Torch v54: (4,) = v53 * 1.0\n",
      "  Torch v55: (4,) = v53 * 1.0\n",
      "  Torch v56: (4,) = v55 * v47\n",
      "  Torch v57: (4,) = v55 * v47\n",
      "  Torch v58: (4,) = v56 + v57\n",
      "  Torch v59: (4,) = v54 * v45\n",
      "  Torch v60: (4,) = v54 * v45\n",
      "  Torch v61: (4,) = v59 + v60\n",
      "  Torch v62: (4,) = v58 * 1.0\n",
      "  Torch v63: (4,) = v58 * 1.0\n",
      "  Torch v64: (4,) = v61 + v63\n",
      "  Torch v65: (4,) = v62 * v40\n",
      "  Torch v66: (4,) = v62 * v44\n",
      "  Torch v67: (4,) = v64 * v1\n",
      "  Torch v68: (4,) = v64 * v44\n",
      "  Torch v69: (4,) = v65 + v67\n",
      "  Torch v70: () = v69.sum(dim=(0,))\n",
      "  Torch v71: (1,) = v70.unsqueeze(0)\n",
      "  Torch v72: () = v71.squeeze(0)\n",
      "  Torch v73: (4,) = v66 * 1.0\n",
      "  Torch v74: (4,) = v66 * 1.0\n",
      "  Torch v75: (4,) = v68 + v74\n",
      "------------------------\n",
      "da tensor([2.2222, 4.0728, 2.1373, 3.8498])\n",
      "db tensor([5.0593, 9.4137, 5.2548, 9.4926])\n"
     ]
    }
   ],
   "source": [
    "# turning off create_graph will impede us from seeing the logging lines for\n",
    "# the second backwards, so we turn on logging for Torch to see them\n",
    "d1 = Autograd(Logger(Torch(), name=\"Torch\"), name=\"Autograd1\", create_graph=False)\n",
    "d2 = Autograd(d1, name=\"Autograd2\", create_graph=False)\n",
    "\n",
    "da, db = run_gradients(d2, d1)\n",
    "print(\"da\", da)\n",
    "print(\"db\", db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f343af81",
   "metadata": {},
   "source": [
    "Under what situations might it be profitable to keep the two tapes separate?\n",
    "One guess we might have is if there is another functional transformation\n",
    "wedged between the two autograd transformations.  We would then expect the\n",
    "backwards formula we save to be different between the two tapes.  To do this, I\n",
    "first need to implement batched tensors.\n",
    "\n",
    "One unusual thing about this implementation is that we do not need to wrap\n",
    "tensors to change their sizes; instead, we just override the meaning of\n",
    "size() on the dispatcher to hide batch dimensions.  One case we do not\n",
    "exercise in this example is implicit broadcasting when you combine a tensor\n",
    "that is not batched with a tensor that is batched: without wrappers, a user\n",
    "must explicitly lift (e.g., unsqueeze and expand) tensors they wish to\n",
    "replicate across the batch dimension.  The code below will blindly attempt to\n",
    "reinterpret a tensor as a batched tensor, even when it may not make sense (if\n",
    "there is a size mismatch, however, you will get an assert failure).  Similarly,\n",
    "once you exit a vmap region, all previously vmap'ed tensors \"magically\" become\n",
    "unbatched.  functorch did not pursue this implementation because at the time\n",
    "Tensor.size() was not virtual and thus it was not possible to override (this\n",
    "will be changing soon)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd1d0688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This implementation of Batched only supports inserting a dimension\n",
    "# at the very front\n",
    "class Batched(Dispatcher):\n",
    "    def __init__(self, inner, *, length, name=\"Batched\"):\n",
    "        self.inner = inner\n",
    "        self.name = name\n",
    "        self.length = length\n",
    "\n",
    "    def size(self, input):\n",
    "        sizes = self.inner.size(input)\n",
    "        assert sizes[0] == self.length\n",
    "        return sizes[1:]\n",
    "\n",
    "    def ones(self, size):\n",
    "        return self.inner.ones((self.length,) + size)\n",
    "\n",
    "    def mul(self, lhs, rhs):\n",
    "        assert self.inner.size(lhs)[0] == self.length\n",
    "        if not isinstance(rhs, float):\n",
    "            assert self.inner.size(rhs)[0] == self.length\n",
    "        return self.inner.mul(lhs, rhs)\n",
    "\n",
    "    def add(self, lhs, rhs):\n",
    "        assert self.inner.size(lhs)[0] == self.length\n",
    "        assert self.inner.size(rhs)[0] == self.length\n",
    "        return self.inner.add(lhs, rhs)\n",
    "\n",
    "    def sum(self, input, dim=None, name=None):\n",
    "        # offset all the summed over dimensions by one\n",
    "        assert self.inner.size(input)[0] == self.length\n",
    "        dim = tuple(\n",
    "            i + 1 for i in sum_dims(input_dim=self.inner.dim(input) - 1, dim=dim)\n",
    "        )\n",
    "        return self.inner.sum(input, dim, name=name)\n",
    "\n",
    "    def expand(self, input, sizes):\n",
    "        # offset sizes by one\n",
    "        assert self.inner.size(input)[0] == self.length\n",
    "        return self.inner.expand(input, (self.inner.size(input)[0],) + sizes)\n",
    "\n",
    "    def squeeze(self, input, dim):\n",
    "        # offset dim by one\n",
    "        assert self.inner.size(input)[0] == self.length\n",
    "        return self.inner.squeeze(input, dim + 1)\n",
    "\n",
    "    def unsqueeze(self, input, dim):\n",
    "        # offset dim by one\n",
    "        assert self.inner.size(input)[0] == self.length\n",
    "        return self.inner.unsqueeze(input, dim + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dffb9a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autograd1 v78: (2, 4) = v76 + v77\n",
      "Autograd3 v78: (4,) = v76 + v77\n",
      "Autograd1 v79: (2, 4) = v78 * v77\n",
      "Autograd3 v79: (4,) = v78 * v77\n",
      "Autograd1 L0: (2,) = v79.sum(dim=(1,))\n",
      "Autograd3 L0: () = v79.sum(dim=0)\n",
      "-- Autograd3 dL0 -------\n",
      "Autograd1 v81: (2, 1) = v80.unsqueeze(dim=1)\n",
      "Autograd1 v82: (2, 4) = v81.expand((2, 4))\n",
      "Autograd1 v83: (2, 4) = v82 * v77\n",
      "Autograd1 v84: (2, 4) = v82 * v78\n",
      "Autograd1 v85: (2, 4) = v84 + v83\n",
      "------------------------\n",
      "Autograd1 v86: (2, 4) = v83 * v83\n",
      "Autograd1 v87: (2, 4) = v85 * v85\n",
      "Autograd1 v88: (2, 4) = v86 + v87\n",
      "Autograd1 L1: () = v88.sum(dim=None)\n",
      "-- Autograd1 dL1 -------\n",
      "------------------------\n",
      "va tensor([[0.4556, 0.6323, 0.3489, 0.4017],\n",
      "        [0.0223, 0.1689, 0.2939, 0.5185]])\n",
      "vb tensor([[0.6977, 0.8000, 0.1610, 0.2823],\n",
      "        [0.6816, 0.9152, 0.3971, 0.8742]])\n",
      "dva tensor([[3.7019, 4.4647, 1.3419, 1.9325],\n",
      "        [2.7711, 3.9985, 2.1762, 4.5337]])\n",
      "dvb tensor([[ 8.7992, 10.5293,  3.0059,  4.4296],\n",
      "        [ 6.9054,  9.8274,  5.1466, 10.8156]])\n"
     ]
    }
   ],
   "source": [
    "# Our inputs are batched this time!\n",
    "va, vb = label(torch.rand(2, 4)), label(torch.rand(2, 4))\n",
    "\n",
    "d1 = Autograd(Torch(), name=\"Autograd1\", create_graph=False)\n",
    "d2 = Batched(d1, length=2, name=\"Batched2\")\n",
    "d3 = Autograd(d2, name=\"Autograd3\", create_graph=False)\n",
    "\n",
    "\n",
    "def run_batched_gradients(d3, d2, d1):\n",
    "    # our first loss\n",
    "    # we write the dimension we reduce on explicitly for clarity\n",
    "    L0 = d3.sum(simple(d3, va, vb), dim=0, name=\"L0\")\n",
    "\n",
    "    # compute derivatives of our inputs\n",
    "    dL0_da, dL0_db = d3.grad(L0, [va, vb])\n",
    "\n",
    "    # now lets compute the L2 norm of our derivatives\n",
    "    L1 = d1.sum(d1.add(d1.mul(dL0_da, dL0_da), d1.mul(dL0_db, dL0_db)), name=\"L1\")\n",
    "\n",
    "    # and take the gradient of that.\n",
    "    # notice there are two losses involved1.\n",
    "    dL1_da, dL1_db = d1.grad(L1, [va, vb])\n",
    "    return dL1_da, dL1_db\n",
    "\n",
    "\n",
    "dva, dvb = run_batched_gradients(d3, d2, d1)\n",
    "print(\"va\", va)\n",
    "print(\"vb\", vb)\n",
    "print(\"dva\", dva)\n",
    "print(\"dvb\", dvb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66d013e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "To see that we have done this correctly, we could run the corresponding JAX:\n",
    "\n",
    "```\n",
    "from jax import grad, vmap\n",
    "import jax.numpy as np\n",
    "\n",
    "def simple(a, b):\n",
    "  t = a + b\n",
    "  return t * b\n",
    "\n",
    "def L0(a, b):\n",
    "  return np.sum(simple(a, b))\n",
    "\n",
    "def L1(a, b):\n",
    "  dL0_da, dL0_db = vmap(grad(L0, argnums=(0,1)), in_axes=0)(a, b)\n",
    "  return (dL0_da * dL0_da + dL0_db * dL0_db).sum()\n",
    "\n",
    "va = np.asarray([[0.4556, 0.6323, 0.3489, 0.4017],\n",
    "        [0.0223, 0.1689, 0.2939, 0.5185]])\n",
    "vb = np.asarray([[0.6977, 0.8000, 0.1610, 0.2823],\n",
    "        [0.6816, 0.9152, 0.3971, 0.8742]])\n",
    "dva, dvb = grad(L1, argnums=(0,1))(va, vb)\n",
    "print(\"dva\", dva)\n",
    "print(\"dvb\", dvb)\n",
    "```\n",
    "\n",
    "Looking over the output, the tapes look similar, but we can see that the sizes\n",
    "and the arguments of the operations in question differ (after all, Autograd3 is\n",
    "on the inside of the vmap, while Autograd1 is outside).  But it is still very\n",
    "similar: we could imagine simply varying the dispatcher we use to process backwards\n",
    "depending on when we are executing the tape.  In fact, this is exactly what an\n",
    "initial, non-functorch implementation of PyTorch did to support per-sample\n",
    "gradients.\n",
    "\n",
    "Exercise: modify Autograd.grad to accept a dispatcher, and use that dispatcher\n",
    "instead of self.backward_inner() when running propagator functions.  Then, rewrite\n",
    "the above example so that it only has one level of Autograd:\n",
    "Batched(Autograd(Torch(), create_graph=True)) and show you still get the same\n",
    "result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2706fca7",
   "metadata": {},
   "source": [
    "OK, so all of this dispatcher business is all nice and explicit, but\n",
    "that's not what JAX/functorch's interface looks like.  How do we\n",
    "bridge the gap?\n",
    "\n",
    "TODO: write this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d9f0bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76f9d0e9",
   "metadata": {},
   "source": [
    "Epilogue: Why didn't I use inheritance?  In fact, in the first version\n",
    "of this notebook, I did.  But self makes it easy to accidentally go\n",
    "back to the \"top\" of the dispatch stack, which is typically not what\n",
    "you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80652b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"All done\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
